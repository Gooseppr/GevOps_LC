---
layout: page
title: Kubernetes avanc√©s
sujet: Orchestration
type: module
jour: 14
ordre: 1
tags: kubernetes, kubectl, yaml
---

# üß† Kubernetes Avanc√© (Jour 2)

Comprendre vraiment les objets et les manifests YAML

---

## 0. Rappel : comment Kubernetes lit un fichier YAML

Quand tu fais :

```bash
kubectl apply -f monfichier.yaml

```

Kubernetes lit le YAML et voit 4 choses essentielles :

1. **apiVersion**
    
    ‚Üí Quelle version de l‚ÄôAPI Kubernetes doit traiter cet objet.
    
    Exemple : `v1`, `apps/v1`, `batch/v1`‚Ä¶
    
    (Chaque groupe d‚Äôobjets est g√©r√© par un ‚Äúgroupe d‚ÄôAPI‚Äù.)
    
2. **kind**
    
    ‚Üí Quel type d‚Äôobjet tu veux cr√©er.
    
    Exemple : `Pod`, `Deployment`, `Service`, `Secret`, etc.
    
3. **metadata**
    
    ‚Üí L‚Äôidentit√© de l‚Äôobjet : nom, labels, namespace.
    
4. **spec**
    
    ‚Üí Le ‚Äúplan de construction‚Äù.
    
    C‚Äôest l‚Äô√©tat d√©sir√©. Ce que TU veux.
    
    Kubernetes essaie ensuite de le faire respecter.
    

üí° Tr√®s important :

Le contenu exact de `spec` d√©pend du type d‚Äôobjet (`kind`).

Un `Pod` n‚Äôa pas la m√™me `spec` qu‚Äôun `Deployment`, parce qu‚Äôils ne d√©crivent pas la m√™me chose.

C‚Äôest pour √ßa que tu vois des `spec` imbriqu√©s : un objet peut d√©crire un autre objet √† l‚Äôint√©rieur, et chacun a sa propre `spec`.

On va le voir concr√®tement.

---

## 1. Pod

### Qu‚Äôest-ce qu‚Äôun Pod ?

Un **Pod** est l‚Äôunit√© d‚Äôex√©cution minimale dans Kubernetes.

C‚Äôest l√† o√π tes conteneurs tournent r√©ellement.

Un Pod peut contenir :

- 1 conteneur (cas le plus courant),
- ou plusieurs conteneurs qui travaillent ensemble (sidecar, agent, etc.).

Un Pod est **√©ph√©m√®re** : s‚Äôil meurt, Kubernetes peut le recr√©er‚Ä¶ mais ce ne sera pas ‚Äúle m√™me‚Äù, ce sera un nouveau Pod du m√™me mod√®le.

---

### Exemple YAML de Pod simple

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80

```

### Lecture ligne par ligne

- `apiVersion: v1`
    
    ‚Üí Les Pods sont g√©r√©s par l‚ÄôAPI ‚Äúcore‚Äù de Kubernetes, groupe de base.
    
    Pour les Pods c‚Äôest juste `v1`.
    
- `kind: Pod`
    
    ‚Üí On d√©clare qu‚Äôon veut cr√©er un objet de type Pod.
    
- `metadata:`
    - `name: web-pod`
        
        Nom du Pod dans le cluster. Tu t‚Äôen serviras pour `kubectl describe pod web-pod`.
        
    - `labels:`
        - `app: web`
            
            Les labels sont des √©tiquettes. Ils servent plus tard pour faire du ciblage.
            
            Par exemple, un Service pourra dire : ‚Äúenvoie le trafic √† tous les Pods qui ont `app=web`‚Äù.
            
- `spec:`
    
    ‚Üí Voil√† l‚Äô√©tat d√©sir√© de CE pod.
    
    - `containers:`
        
        ‚Üí Liste des conteneurs dans ce Pod.
        
        - `name: nginx-container`
            
            Nom logique du conteneur.
            
        - `image: nginx:latest`
            
            L‚Äôimage Docker/OCI √† ex√©cuter.
            
        - `ports:`
            - `containerPort: 80`
                
                Le port expos√© DANS le Pod.
                

üí° Ici, il n‚Äôy a qu‚Äôun seul `spec`, parce que le Pod est la ressource finale : pas besoin d‚Äôaller plus loin.

---

## 2. Deployment

### C‚Äôest quoi un Deployment ?

Un **Deployment** g√®re la vie d‚Äôun groupe de Pods identiques :

- combien d‚Äôinstances doivent tourner en parall√®le,
- comment les mettre √† jour sans coupure (rolling update),
- comment revenir en arri√®re (rollback).

Le Deployment ne lance pas les Pods directement.

Il g√®re un objet sous-jacent qui s‚Äôappelle ReplicaSet, qui lui g√®re les Pods.

Donc tu vas voir des `spec` imbriqu√©s : Deployment ‚Üí ReplicaSet ‚Üí Pod.

---

### Exemple YAML de Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.27
        ports:
        - containerPort: 80

```

### Lecture ligne par ligne

- `apiVersion: apps/v1`
    
    ‚Üí Les Deployments sont g√©r√©s par le groupe d‚ÄôAPI `apps`.
    
    Pas par `v1`. Donc `apps/v1`.
    
    (C‚Äôest historique : les objets plus ‚Äú√©volu√©s‚Äù vivent souvent dans des groupes API sp√©cialis√©s.)
    
- `kind: Deployment`
    
    ‚Üí Tu demandes un objet de type Deployment.
    
- `metadata:`
    - `name: web-deployment`
        
        ‚Üí Nom du Deployment.
        
    - `labels:`
        
        ‚Üí Labels du Deployment lui-m√™me (souvent utilis√© pour organisation, pas pour le trafic).
        
- `spec:`
    
    ‚Üí C‚Äôest LA d√©finition de l‚Äô√©tat d√©sir√© du Deployment.
    
    - `replicas: 3`
        
        ‚Üí Tu veux 3 Pods identiques en m√™me temps.
        
    - `selector:`
        
        ```yaml
        selector:
          matchLabels:
            app: web
        
        ```
        
        ‚Üí Le Deployment doit savoir ‚Äúquels Pods sont √† moi ?‚Äù.
        
        Il utilise ce s√©lecteur pour faire le lien :
        
        tous les Pods qui ont `app: web` sont consid√©r√©s comme g√©r√©s par ce Deployment.
        
        ‚ö†Ô∏è si tu changes √ßa sans changer les labels des Pods dans le template, c‚Äôest le chaos.
        
    - `template:`
        
        ‚Üí Et voil√† le moment important.
        
        Le Deployment d√©crit le mod√®le (= template) du Pod qu‚Äôil va cr√©er.
        
        C‚ÄôEST ICI qu‚Äôon retrouve un **autre objet de type Pod**, imbriqu√©.
        
        - `metadata:`
            - `labels: app: web`
                
                C‚Äôest capital : √ßa doit matcher `selector.matchLabels`.
                
                Sinon le Deployment ne ‚Äúreconna√Æt‚Äù pas ses propres Pods.
                
        - `spec:`
            
            ‚Üí Maintenant on est dans la `spec` du **Pod** qui sera cr√©√©.
            
            C‚Äôest la m√™me logique que dans la ressource Pod pr√©c√©dente :
            
            - conteneurs
            - image
            - ports

üß† Pourquoi on a `spec` dans une `spec` ?

- Le `spec` du **Deployment** explique comment on g√®re l‚Äôensemble (scaling, s√©lection, template).
- Le `spec` du `template` explique √† quoi ressemble CHAQUE Pod concr√®tement.

Tu peux lire comme :

> ‚ÄúJe veux 3 Pods. Voil√† √† quoi doit ressembler chaque Pod.‚Äù
> 

---

## 3. ReplicaSet

### C‚Äôest quoi ?

Un **ReplicaSet** garantit ‚Äúil doit toujours y avoir N Pods qui tournent‚Äù.

Il surveille et recr√©e si besoin.

En vrai :

- Tu n‚Äô√©cris presque jamais un ReplicaSet √† la main.
- Le Deployment cr√©e le ReplicaSet pour toi.
- Le ReplicaSet cr√©e les Pods.

Mais p√©dagogiquement, on va le lire.

---

### Exemple YAML de ReplicaSet

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: web-replicaset
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.27
        ports:
        - containerPort: 80

```

C‚Äôest tr√®s proche du Deployment, sauf :

- Il n‚Äôy a PAS de strat√©gie de mise √† jour (pas de rolling update).
- Il ne g√®re pas l‚Äôhistorique/rollback.
- Il r√©p√®te juste : ‚Äú3 Pods, pareil, toujours‚Äù.

üí° Retiens :

**Deployment = ReplicaSet + Strat√©gie de d√©ploiement.**

---

## 4. StatefulSet

### Pourquoi √ßa existe ?

`Deployment` est parfait pour des applis sans √©tat (stateless).

Mais pour une base de donn√©es ?

Tu veux que :

- chaque instance garde son identit√©,
- les noms ne bougent pas,
- les volumes restent associ√©s √† la bonne instance.

C‚Äôest le r√¥le du **StatefulSet** :

- identit√©s stables (`pod-0`, `pod-1`, ‚Ä¶),
- ordre de d√©marrage contr√¥l√©,
- stockage persistant attach√© par Pod.

---

### Exemple YAML de StatefulSet

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
spec:
  serviceName: "db-service"
  replicas: 2
  selector:
    matchLabels:
      app: mydb
  template:
    metadata:
      labels:
        app: mydb
    spec:
      containers:
      - name: postgres
        image: postgres:16
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi

```

### Lecture ligne par ligne

- `apiVersion: apps/v1`
    
    StatefulSet fait partie du groupe `apps`.
    
- `kind: StatefulSet`
    
    OK.
    
- `metadata.name: db`
    
    Le nom du StatefulSet.
    
- `spec.serviceName: "db-service"`
    
    Important : les Pods vont √™tre accessibles via un Service ‚Äúheadless‚Äù interne.
    
    Chaque Pod aura un DNS stable bas√© sur ce service.
    
- `replicas: 2`
    
    On veut 2 Pods : `db-0`, `db-1`.
    
- `selector.matchLabels / template.metadata.labels`
    
    M√™me logique qu‚Äôavec le Deployment : il faut que √ßa matche.
    
- `template.spec.containers`
    
    L√† c‚Äôest le Pod mod√®le : comme avant.
    
- `volumeMounts`
    
    Le conteneur monte un volume nomm√© `data` dans `/var/lib/postgresql/data`.
    
- `volumeClaimTemplates:`
    
    Cette section est **sp√©cifique au StatefulSet**.
    
    Kubernetes va cr√©er un PVC INDIVIDUEL par Pod, par exemple :
    
    - `data-db-0`
    - `data-db-1`

√áa garantit que `db-0` garde SES donn√©es m√™me si le Pod est red√©ploy√© ailleurs.

---

## 5. DaemonSet

### Pourquoi ?

Un **DaemonSet** dit √† Kubernetes :

> ‚ÄúCe Pod doit tourner sur CHAQUE n≈ìud du cluster.‚Äù
> 

Cas typiques :

- Agent de logs,
- Agent de monitoring,
- Outil r√©seau.

---

### Exemple YAML de DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-logger
spec:
  selector:
    matchLabels:
      app: logger
  template:
    metadata:
      labels:
        app: logger
    spec:
      containers:
      - name: log-agent
        image: my-logger:latest
        resources:
          limits:
            cpu: "100m"
            memory: "128Mi"

```

### Lecture

- `apiVersion: apps/v1`, `kind: DaemonSet`
- Pas de `replicas` ici.
    
    ‚Üí Parce que le nombre de Pods d√©pend du nombre de n≈ìuds du cluster, pas d‚Äôun chiffre que tu choisis.
    
- `template.spec.containers...`
    
    Le Pod mod√®le √† lancer sur chaque n≈ìud.
    

üß† Ajout automatique :

- Tu ajoutes un nouveau n≈ìud au cluster ‚Üí Kubernetes d√©ploie automatiquement ce Pod sur le nouveau n≈ìud.
- Tu retires un n≈ìud ‚Üí le Pod part avec.

---

## 6. Service

### Pourquoi un Service ?

Les Pods ont des IPs qui changent tout le temps (ils meurent, reviennent, etc.).

Un **Service** fournit une **adresse stable** et fait du **load balancing** vers les Pods qui matchent certains labels.

---

### Exemple YAML de Service NodePort

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080

```

### Lecture

- `apiVersion: v1`
    
    Les Services sont dans l‚ÄôAPI core ‚Üí `v1`.
    
- `kind: Service`
- `metadata.name: web-service`
- `spec.type: NodePort`
    
    ‚Üí On expose le service sur un port ouvert de chaque n≈ìud du cluster.
    
    Utile pour tester depuis l‚Äôext√©rieur en labo/minikube.
    
- `selector.app: web`
    
    ‚Üí ‚ÄúEnvoie le trafic vers les Pods qui ont le label `app=web`.‚Äù
    
    C‚Äôest l√† que les labels deviennent vitaux.
    
- `ports:`
    - `port: 80`
        
        Le port du Service (ce que les autres Pods du cluster vont viser).
        
    - `targetPort: 80`
        
        Le port √† l‚Äôint√©rieur du Pod.
        
    - `nodePort: 30080`
        
        Le port ouvert sur les n≈ìuds pour acc√®s externe.
        

---

### Les 3 types principaux de Service

| Type | Utilisation |
| --- | --- |
| `ClusterIP` | Interne au cluster uniquement (par d√©faut) |
| `NodePort` | Expose via un port sur chaque n≈ìud |
| `LoadBalancer` | S‚Äôint√®gre √† un load balancer cloud public / externe |

---

## 7. Ingress

### √Ä quoi √ßa sert ?

Un **Ingress** g√®re le **routage HTTP/HTTPS** vers diff√©rents Services internes.

Il permet aussi :

- la gestion des noms de domaine,
- le HTTPS (TLS),
- les chemins `/api`, `/front`, etc.

Il travaille AVEC un **Ingress Controller** (par exemple Nginx ou Traefik).

---

### Exemple YAML d‚ÄôIngress

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  rules:
  - host: myapp.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80

```

### Lecture

- `apiVersion: networking.k8s.io/v1`
    
    ‚Üí L‚Äôingress appartient au groupe API `networking.k8s.io`.
    
- `kind: Ingress`
- `spec.rules`
    
    Liste des r√®gles de routage.
    
    - `host: myapp.local`
        
        Nom de domaine attendu dans la requ√™te.
        
    - `paths:`
        - `path: /`
            
            Toutes les requ√™tes sur `/` (et sous-chemins avec `Prefix`)‚Ä¶
            
        - `backend.service.name: web-service`
            
            ‚Ä¶ sont envoy√©es vers le Service `web-service`.
            

üí° Donc :

Client ‚Üí Ingress ‚Üí Service ‚Üí Pods.

---

## 8. ConfigMap

### R√¥le

Un **ConfigMap** stocke de la configuration NON SENSIBLE (cl√©/valeur).

But : ne pas re-builder l‚Äôimage juste pour changer une variable.

---

### YAML d‚Äôun ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_MODE: "production"
  NGINX_VERSION: "1.2.4"

```

### Injection dans un Pod (extrait)

```yaml
env:
- name: APP_MODE
  valueFrom:
    configMapKeyRef:
      name: app-config
      key: APP_MODE

```

### Lecture

- `data:`
    
    C‚Äôest un dictionnaire cl√© ‚Üí valeur (tout en texte).
    
- Ensuite tu peux injecter ces valeurs dans le Pod via `env.valueFrom.configMapKeyRef`.

---

## 9. Secret

### R√¥le

Un **Secret** stocke des valeurs sensibles (mots de passe, tokens).

M√™me principe qu‚Äôun ConfigMap, mais s√©curis√©.

---

### YAML d‚Äôun Secret (type g√©n√©rique)

‚ö† Les valeurs sont encod√©es en base64 dans les manifests classiques.

(ce n‚Äôest PAS un vrai chiffrement, c‚Äôest juste pour ne pas les afficher en clair)

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  DB_USER: ZGV2ZWxvcGVy
  DB_PASSWORD: c2VjcmV0MTIz

```

Ici :

- `ZGV2ZWxvcGVy` = "developer" encod√© base64
- `c2VjcmV0MTIz` = "secret123" encod√© base64

Injection dans un Pod :

```yaml
env:
- name: DB_PASSWORD
  valueFrom:
    secretKeyRef:
      name: db-secret
      key: DB_PASSWORD

```

---

## 10. Volumes persistants : PV & PVC

### Id√©e

- **PV (PersistentVolume)** : ressource de stockage dispo dans le cluster (disque, NFS, cloud disk‚Ä¶).
- **PVC (PersistentVolumeClaim)** : demande de stockage faite par une appli.

Le Pod ne demande pas ‚Äúdonne-moi ce disque-l√†‚Äù, il dit ‚Äúj‚Äôai besoin de 5Go en lecture-√©criture‚Äù, et Kubernetes fait l‚Äôassociation.

---

### Exemple PV + PVC

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-data
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: /data/db
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-data
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

```

### Lecture

- PV :
    - `capacity.storage: 10Gi` ‚Üí taille dispo
    - `hostPath.path: /data/db` ‚Üí o√π √ßa vit physiquement (ici: stockage local du n≈ìud, pas pour la prod mais utile en labo)
- PVC :
    - `requests.storage: 5Gi` ‚Üí ‚Äúje veux 5 Go‚Äù
    - Kubernetes va essayer d‚Äôattacher ce PVC √† un PV compatible.

Ensuite dans ton Pod :

```yaml
volumes:
- name: data
  persistentVolumeClaim:
    claimName: pvc-data

```

---

## 11. Jobs et CronJobs

### Job

Un **Job** ex√©cute une t√¢che et doit aller jusqu‚Äôau bout (ex: migration DB, g√©n√©ration d‚Äôun rapport).

Quand c‚Äôest termin√© avec succ√®s, c‚Äôest fini.

### CronJob

Un **CronJob** lance des Jobs r√©guli√®rement (toutes les heures, tous les jours √† 03:00, etc.), comme `cron` en Linux.

---

### Exemple CronJob

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-job
spec:
  schedule: "0 3 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleaner
            image: alpine
            command: ["sh", "-c", "echo Nettoyage termin√©"]
          restartPolicy: OnFailure

```

### Lecture

- `apiVersion: batch/v1`
    
    ‚Üí Les Jobs/CronJobs vivent dans le groupe d‚ÄôAPI `batch`.
    
- `kind: CronJob`
- `spec.schedule: "0 3 * * *"`
    
    ‚Üí Syntaxe cron : ici, tous les jours √† 03h00.
    
- `jobTemplate:`
    
    ‚Üí Mod√®le du Pod qui sera lanc√© √† chaque ex√©cution planifi√©e.
    
- `restartPolicy: OnFailure`
    
    ‚Üí Si √ßa √©choue, r√©essaie.
    

---

## 12. HPA (Horizontal Pod Autoscaler)

### R√¥le

L‚ÄôHPA ajuste le nombre de Pods automatiquement selon l‚Äôusage CPU/m√©moire.

Exemple : si la charge CPU d√©passe 80%, passe de 3 Pods √† 6 Pods.

Si la charge redescend, reviens √† 3 Pods.

---

### Exemple YAML d‚ÄôHPA (classique bas√© CPU)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80

```

### Lecture

- `apiVersion: autoscaling/v2`
    
    ‚Üí Les HPA vivent dans le groupe d‚ÄôAPI `autoscaling`.
    
- `scaleTargetRef`
    
    ‚Üí √Ä quoi on applique l‚Äôauto-scale ?
    
    Ici : le Deployment `web-deployment`.
    
- `minReplicas` / `maxReplicas`
    
    ‚Üí Fourchette autoris√©e.
    
- `averageUtilization: 80`
    
    ‚Üí Si l‚Äôutilisation CPU moyenne par Pod d√©passe 80%, on scale OUT.
    

üß† Ce m√©canisme = scalabilit√© automatique sans intervention humaine.

---

## 13. R√©cap visuel : qui fait quoi ?

| Objet | API Version | R√¥le principal |
| --- | --- | --- |
| Pod | v1 | Conteneur(s) en ex√©cution r√©elle |
| Deployment | apps/v1 | G√®re le d√©ploiement, le rollout, le rollback, le scaling |
| ReplicaSet | apps/v1 | Maintient le bon nombre de Pods |
| StatefulSet | apps/v1 | Pods √† identit√© stable + stockage par instance |
| DaemonSet | apps/v1 | Un Pod par n≈ìud (logs, monitoring, r√©seau) |
| Service | v1 | IP stable + load balancing entre Pods |
| Ingress | networking.k8s.io/v1 | Routage HTTP/HTTPS et domaines externes |
| ConfigMap | v1 | Variables non sensibles (cl√©/valeur) |
| Secret | v1 | Secrets chiffr√©s/base64, mots de passe, tokens |
| PersistentVolume | v1 | Ressource de stockage physique disponible |
| PVC | v1 | Demande de stockage par une appli |
| Job / CronJob | batch/v1 | T√¢ches ponctuelles ou r√©currentes |
| HPA | autoscaling/v2 | Auto-scale horizontal selon la charge |