---
title: Projet J2 - Ansible avanc√©
sujet: Projet
type: module
jour: 42
ordre: 1
tags: projet, docker, docker swarm, stack, ansible
---

# üöÄ Passage √† Ansible ‚Äúpro‚Äù pour le d√©ploiement NocoDB + Postgres

Objectif de la journ√©e :

partir du playbook **fonctionnel mais basique** (`swarm.yml`) pour arriver √† une version **plus robuste, plus modulable et plus s√©curis√©e** (`swarm-pro.yml`, `inventory-pro.ini`, `.env.j2`, `vault.yml`).

---

## 1. Rappel tr√®s rapide de la V1 ‚Äúsimple‚Äù

La V1 faisait d√©j√† le job :

- configuration des hostnames,
- installation de Docker,
- init du Swarm sur le manager,
- join du worker,
- ajout des labels,
- copie de `compose.yml` + `.env`,
- `docker stack deploy ‚Ä¶`.

Mais :

- le `.env` √©tait en clair dans le repo,
- pas de **tags** ‚Üí difficile de rejouer une partie seulement,
- pas de **gestion d‚Äôerreur structur√©e** (`block/rescue`),
- pas de g√©n√©ration dynamique de `.env`.

---

## 2. Ce qu'on a ajout√© dans la V2 ‚Äúpro‚Äù

### 2.1. Des tags pour piloter le playbook comme un pro

On a rajout√© des **tags** sur chaque bloc du playbook : 
swarm-pro

```yaml
- name: 0) Configurer les hostnames Linux
  hosts: swarm_all
  become: true
  tags: [hostnames]

- name: 1. Installer Docker sur toutes les VMs
  hosts: swarm_all
  become: true
  tags: [docker]

- name: 2. Initialiser le Swarm sur le manager
  hosts: swarm_managers
  become: true
  tags: [swarm]

- name: 3. Joindre les workers au Swarm
  hosts: swarm_workers
  become: true
  tags: [swarm, swarm_join]

- name: 4) Ajouter les labels sur les n≈ìuds (depuis le manager)
  hosts: swarm_all
  become: true
  tags: [swarm, labels]

- name: 5. D√©ployer la stack NocoDB + Postgres
  hosts: swarm_managers
  become: true
  tags: [deploy]

```

üëâ Ce que √ßa t‚Äôapporte :

- Rejouer **uniquement ce qui t‚Äôint√©resse**, par exemple :

```bash
ansible-playbook -i inventory-pro.ini swarm-pro.yml --tags deploy
ansible-playbook -i inventory-pro.ini swarm-pro.yml --tags swarm_join

```

- En prod, on pourra :
    - r√©installer Docker sans toucher au Swarm,
    - red√©ployer la stack sans casser les n≈ìuds,
    - relancer seulement les labels si on ajoutes des workers.

---

### 2.2. Ajout de `block / rescue` pour un join & un deploy plus robustes

On a introduit des blocs **`block` / `rescue`** sur deux moments critiques :

1. le **join des workers au Swarm**,
2. le **d√©ploiement de la stack**.

### a) Join des workers avec bloc de secours

```yaml
- name: 3. Joindre les workers au Swarm
  hosts: swarm_workers
  become: true
  tags: [swarm, swarm_join]
  vars:
    manager_name: "{{ groups['swarm_managers'][0] }}"
    manager_ip: "{{ hostvars[manager_name].swarm_manager_ip | default(hostvars[manager_name].ansible_default_ipv4.address) }}"
    worker_token: "{{ hostvars[manager_name].swarm_worker_token }}"
  tasks:
    - block:
        - name: V√©rifier l'√©tat local du Swarm sur le worker
          shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
          register: worker_swarm_state
          changed_when: false
          failed_when: false

        - name: Quitter un Swarm existant sur le worker (si d√©j√† actif)
          shell: docker swarm leave --force
          when: worker_swarm_state.stdout is defined and worker_swarm_state.stdout | trim == "active"

        - name: Joindre le Swarm en tant que worker
          shell: docker swarm join --token {{ worker_token }} {{ manager_ip }}:2377
          register: join_result
          when: worker_swarm_state.stdout is not defined or worker_swarm_state.stdout | trim != "active"
          retries: 5
          delay: 5
          until: join_result.rc == 0 or 'already part of a swarm' in join_result.stderr
          failed_when: >
            join_result.rc != 0 and
            'already part of a swarm' not in join_result.stderr

        - name: V√©rifier c√¥t√© worker que le Swarm est actif apr√®s join
          shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
          register: worker_swarm_state_final
          changed_when: false

        - name: √âchouer si le Swarm n'est toujours pas actif sur le worker
          fail:
            msg: "Le Swarm n'est pas actif sur le worker {{ inventory_hostname }} (√©tat={{ worker_swarm_state_final.stdout | trim }})."
          when: worker_swarm_state_final.stdout | trim != "active"

        - name: Attendre que le n≈ìud soit visible c√¥t√© manager et Ready/Active
          delegate_to: "{{ manager_name }}"
          shell: docker node ls --format '{{ "{{ .Hostname }} {{ .Status }} {{ .Availability }}" }}'
          register: node_list
          retries: 12
          delay: 5
          until: node_list.stdout is search(inventory_hostname + " Ready Active")
          changed_when: false
          failed_when: node_list.stdout is not search(inventory_hostname + " Ready Active")

      rescue:
        - debug:
            msg: "√âchec lors du join du worker {{ inventory_hostname }} au Swarm"
        - fail:
            msg: "Arr√™t du playbook car le worker {{ inventory_hostname }} n'a pas pu rejoindre le Swarm."

```

**Probl√®mes rencontr√©s :**

- Plusieurs erreurs de type :
    - `unexpected parameter type in action`,
    - `no module/action detected in task`,
    - `conflicting action statements`.
- En r√©alit√©, c‚Äô√©tait **uniquement de la syntaxe YAML / Ansible** :
    - `block:` doit √™tre une t√¢che √† part enti√®re,
    - `rescue:` doit √™tre align√© sous `block` (m√™me indentation que les t√¢ches du `block`),
    - chaque t√¢che dans `rescue` doit avoir un `name` + le module (`debug`, `fail`, ‚Ä¶).

üëâ Ce que √ßa t‚Äôapporte :

- Si un worker ne rejoint pas correctement le Swarm, on a maintenant :
    - un message clair (`debug`),
    - un `fail` explicite qui stoppe tout le playbook au lieu de continuer √† demi-cass√©.

### b) D√©ploiement de la stack avec rollback logique

Sur la partie d√©ploiement : 
swarm-pro

```yaml
- name: 5. D√©ployer la stack NocoDB + Postgres
  hosts: swarm_managers
  become: true
  tags: [deploy]
  vars:
    stack_name: mystack
    deploy_path: /opt/nocodb
  tasks:
    - name: Cr√©er le r√©pertoire de d√©ploiement
      file:
        path: "{{ deploy_path }}"
        state: directory
        mode: '0755'

    - name: Copier compose.yml sur le manager
      copy:
        src: files/compose.yml
        dest: "{{ deploy_path }}/compose.yml"
        mode: '0644'

    - name: G√©n√©rer le .env depuis le template et les vars (dont Vault)
      template:
        src: files/.env.j2
        dest: "{{ deploy_path }}/.env"
        mode: '0640'

    - block:
        - name: D√©ployer la stack Docker Swarm
          shell: docker stack deploy -c {{ deploy_path }}/compose.yml {{ stack_name }}
          args:
            chdir: "{{ deploy_path }}"

        - name: Afficher les services de la stack
          shell: docker stack services {{ stack_name }}
          register: stack_services

        - name: Debug - services de la stack
          debug:
            var: stack_services.stdout

      rescue:
        - debug:
            msg: "√âchec lors du d√©ploiement de la stack {{ stack_name }} sur {{ inventory_hostname }}"
        - fail:
            msg: "Arr√™t du playbook car le d√©ploiement de la stack {{ stack_name }} a √©chou√©."

```

üëâ R√©sultat :

- Si `docker stack deploy` foire (erreur YAML, stack cass√©e, r√©seau inexistant‚Ä¶),
    
    on voit tout de suite **o√π** √ßa coince et le playbook s‚Äôarr√™te avec un message propre.
    

---

### 2.3. Passage du `.env` en clair au `.env.j2` + Vault

Avant, on avait un fichier `.env` **en dur** dans le repo : 
.env

```
POSTGRES_HOST=root_db
POSTGRES_DB=nocodb_db
POSTGRES_USER=admin
POSTGRES_PASSWORD=passwordS√©cur1t√©
NC_DB=pg://root_db:5432?u=admin&p=passwordS√©cur1t√©&d=nocodb_db

```

Maintenant on a :

- un **template** `.env.j2` dans `files/`,
- des **variables sensibles** dans `group_vars/all/vault.yml` (chiffr√©),
- une t√¢che `template` qui g√©n√®re le `.env` sur le manager. swarm-pro

Exemple logique de `.env.j2` (sans d√©voiler les secrets) :

```
POSTGRES_HOST={{ postgres_host }}
POSTGRES_DB={{ postgres_db }}
POSTGRES_USER={{ postgres_user }}
POSTGRES_PASSWORD={{ postgres_password }}
NC_DB=pg://{{ postgres_host }}:5432?u={{ postgres_user }}&p={{ postgres_password }}&d={{ postgres_db }}

```

Et c√¥t√© Vault (fichier chiffr√©) :

```yaml
postgres_host: root_db
postgres_db: nocodb_db
postgres_user: admin
postgres_password: "motDePasseSuperSecret"

```

**Commandes importantes qu'on a utilis√©es :**

```bash
# cr√©er / √©diter le Vault
ansible-vault create group_vars/all/vault.yml
ansible-vault edit group_vars/all/vault.yml

# lancer le playbook en utilisant le Vault
ansible-playbook -i inventory-pro.ini swarm-pro.yml --ask-vault-pass

```

üëâ Ce que √ßa t‚Äôapporte :

- Plus de secrets en clair dans Git.
- Le mot de passe DB peut √™tre modifi√© **sans toucher** au playbook ou au compose :
    - on modifie juste `vault.yml`,
    - on relance le playbook avec `-tags deploy`.

---

### 2.4. R√©utilisation de l‚Äôinventaire et ajout de workers applicatifs

L'inventaire ‚Äúpro‚Äù ressemble √† √ßa : 
inventory-pro

```
[swarm_managers]
app-vm ansible_host=13.37.250.89 ansible_user=admin swarm_role=app ansible_ssh_private_key_file=/workspaces/dev-env/key/web-server.pem

[swarm_workers]
db-vm         ansible_host=35.180.42.29 ansible_user=admin swarm_role=db  ansible_ssh_private_key_file=/workspaces/dev-env/key/web-server.pem
app-replica   ansible_host=13.38.117.254 ansible_user=admin swarm_role=app ansible_ssh_private_key_file=/workspaces/dev-env/key/web-server.pem
app-replica2  ansible_host=35.180.41.200 ansible_user=admin swarm_role=app ansible_ssh_private_key_file=/workspaces/dev-env/key/web-server.pem

[swarm_all:children]
swarm_managers
swarm_workers

```

On a donc :

- 1 manager (`app-vm`, r√¥le `app`),
- 1 worker DB (`db-vm`, r√¥le `db`),
- 2 workers applicatifs (`app-replica`, `app-replica2`, r√¥le `app`).

Ces r√¥les sont utilis√©s par le **play des labels** et par le `placement.constraints` de le `compose.yml`.

R√©sultat :

- `root_db` ne sera d√©ploy√© **que** sur les n≈ìuds `role=db`,
- `nocodb` ne sera d√©ploy√© **que** sur les n≈ìuds `role=app`,
- et on voit maintenant clairement la r√©partition en faisant :

```bash
docker node ls
docker service ps mystack_nocodb
docker service ps mystack_root_db

```

---

### 2.5. Compose toujours simple, mais align√© avec l‚ÄôAnsible ‚Äúpro‚Äù

Le `compose.yml` reste relativement simple, mais il est maintenant **pens√© pour Swarm + Ansible** : 
compose

```yaml
version: "3.9"

networks:
  backend:
    driver: overlay

services:
  root_db:
    image: postgres:16
    env_file:
      - .env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U \"$POSTGRES_USER\" -d \"$POSTGRES_DB\""]
      interval: 10s
      timeout: 2s
      retries: 10
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - backend
    deploy:
      placement:
        constraints:
          - node.labels.role == db

  nocodb:
    image: nocodb/nocodb:latest
    env_file:
      - .env
    ports:
      - "8080:8080"
    volumes:
      - nc_data:/usr/app/data
    networks:
      - backend
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.role == app

volumes:
  db_data: {}
  nc_data: {}

```

Coupl√© √† Ansible, cela donne :

- `.env` g√©n√©r√© dynamiquement en amont,
- r√©seau `backend` automatiquement cr√©√© par `docker stack deploy`,
- bonne r√©partition des services via les labels appliqu√©s par le playbook.

---

## 3. R√©sum√© des gros probl√®mes rencontr√©s pendant la transformation

1. **Erreurs de syntaxe avec `block/rescue`**
    - Mauvaise indentation, `rescue` mal align√©, t√¢ches sans module.
    - Corrig√© en respectant strictement la structure :
        - `block:` (t√¢che),
        - liste de t√¢ches,
        - `rescue:` au m√™me niveau que les t√¢ches du block,
        - `name` + module pour chaque t√¢che du `rescue`.
2. **Variable `worker_token` non d√©finie dans les essais avec d‚Äôautres approches**
    - Finalement le choix a √©t√© de **rester sur du `shell`** pour le join (`docker swarm join ‚Ä¶`) tout en conservant la logique robuste qu'on avait d√©j√†.
3. **Copies de `.env` vs g√©n√©ration**
    - Au d√©but, on recopie `.env` en dur.
    - Migration pas √©vidente vers `.env.j2` + Vault (chemin, droits, template).
    - Maintenant la g√©n√©ration est propre et centralis√©e.

---

## 4. Ce qu'on a gagn√© avec cette V2 ‚ÄúAnsible avanc√©‚Äù

En r√©sum√©, aujourd‚Äôhui on a :

- Un playbook **pilotable par tags** (`hostnames`, `docker`, `swarm`, `swarm_join`, `labels`, `deploy`).
- Une gestion d‚Äôerreurs claire sur les √©tapes critiques (**join** + **deploy**) gr√¢ce √† `block/rescue`.
- Des **secrets sortis du Git** et g√©r√©s proprement via **Ansible Vault** + template `.env.j2`.
- Un inventaire pr√™t pour la **scalabilit√©** (plusieurs workers applicatifs).
- Un d√©ploiement **reproductible, s√©curis√© et √©volutif** pour NocoDB + Postgres sur Docker Swarm.

---
[‚Üê Module pr√©c√©dent](M42_projet-board-J02.md)
---
