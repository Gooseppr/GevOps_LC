---
title: Projet J04 - Ansible avanc√© (r√¥les)
sujet: Projet
type: module
jour: 44
ordre: 0
tags: projet
---

# √âvolution de mon Ansible Swarm ‚Äî de v4 √† v5

Aujourd‚Äôhui j‚Äôai transform√© mon vieux gros playbook ‚Äúone-shot‚Äù en un **vrai projet Ansible structur√©**, avec des r√¥les, des playbooks cibl√©s et une gestion propre de Docker Swarm et de NocoDB.

On r√©sume ici **ce qu'on a chang√©**, **pourquoi on l‚Äôa fait**, et **ce que √ßa nous apporte**.

---

## 0. Contexte : ce que faisait mon ancien playbook (v4)

Avant, on avait un **seul gros `main.yml`** qui encha√Ænait :

- configuration des hostnames,
- installation de Docker avec `apt` et quelques `shell: docker ...`,
- initialisation du Swarm sur le manager,
- join des workers avec des commandes `docker swarm join`,
- ajout de labels via `docker node update`,
- login au registry GitLab avec `docker login` en shell,
- pull de l‚Äôimage NocoDB,
- d√©ploiement de la stack avec `docker stack deploy`,
- un check ‚Äúmaison‚Äù des r√©plicas via `docker service ls`.

Probl√®mes :

- peu **modulaire** (tout dans un seul fichier),
- beaucoup de `shell:` / `command:` pour des choses o√π Ansible a des **modules d√©di√©s**,
- difficile d‚Äôajouter une nouvelle VM ou un nouveau r√¥le sans toucher partout,
- gestion des tokens Swarm fragile (join/leave pas toujours propres),
- difficile √† r√©utiliser dans un pipeline CI/CD.

---

## 1. Restructuration d'Ansible en r√¥les (architecture v5)

On est pass√© √† une architecture **par r√¥les**, avec un playbook orchestrateur.

### 1.1. Les r√¥les principaux

On a maintenant :

- `roles/common`
    
    ‚Üí gestion des **hostnames** et de `/etc/hosts`
    
- `roles/docker`
    
    ‚Üí installation de Docker + Docker SDK Python + login au registry GitLab
    
- `roles/swarm-manager`
    
    ‚Üí initialisation du Swarm et exposition des tokens
    
- `roles/swarm-worker`
    
    ‚Üí join propre des workers au cluster Swarm
    
- `roles/swarm-labels`
    
    ‚Üí ajout de labels `role=app`, `role=db`, etc. sur chaque n≈ìud
    
- `roles/nocodb`
    
    ‚Üí pull de l‚Äôimage NocoDB, g√©n√©ration du `compose.yml` + `.env`, d√©ploiement de la stack et v√©rification des r√©plicas
    

On les appelle depuis un **playbook principal** (par exemple `playbooks/run.yml`) qui ressemble grosso modo √† :

```yaml
- name: Configurer la base commune sur toutes les VMs
  hosts: swarm_all
  become: true
  roles:
    - common

- name: Configurer le manager Swarm
  hosts: swarm_manager
  become: true
  collections:
    - community.docker
  roles:
    - docker
    - swarm-manager
    - nocodb     # d√©ploiement de la stack sur le manager

- name: Configurer les workers Swarm
  hosts: swarm_workers
  become: true
  collections:
    - community.docker
  roles:
    - docker
    - swarm-worker

- name: Ajouter les labels Swarm
  hosts: swarm_all
  become: true
  roles:
    - swarm-labels

```

üëâ √áa me permet de **relancer seulement une partie** facilement, et de faire √©voluer chaque r√¥le ind√©pendamment.

---

## 2. J‚Äôai industrialis√© la partie Docker (r√¥le `docker`)

### 2.1. Installation propre de Docker

Dans le r√¥le `docker`, on fait maintenant :

- installation des pr√©requis :
    
    ```yaml
    apt:
      name:
        - ca-certificates
        - curl
        - gnupg
      state: present
      update_cache: yes
    
    ```
    
- installation de Docker :
    
    ```yaml
    apt:
      name: docker.io
      state: present
    
    ```
    
- activation du service :
    
    ```yaml
    service:
      name: docker
      state: started
      enabled: true
    
    ```
    

### 2.2. Installation du SDK Docker pour Python

Les modules `community.docker` ont besoin du **SDK Docker Python** sur les machines distantes.

On l‚Äôinstalle dans le r√¥le :

```yaml
- name: Installer le SDK Docker pour Python
  ansible.builtin.command:
    cmd: /usr/bin/pip3 install docker --break-system-packages
  changed_when: "'Successfully installed' in command_result.stdout"

```

üëâ √áa √©vite l‚Äôerreur :

> No module named 'docker'
> 

et rend les modules `community.docker.*` utilisables partout.

### 2.3. Login GitLab Registry via `community.docker.docker_login`

Au lieu de faire un `shell: docker login ...`, j‚Äôutilise maintenant :

```yaml
- name: Login au registry GitLab avec community.docker
  community.docker.docker_login:
    registry_url: "{{ gitlab_registry_url }}"
    username: "{{ gitlab_registry_user }}"
    password: "{{ gitlab_registry_token }}"
    reauthorize: true
    state: present

```

Les valeurs viennent de mon **vault** : `group_vars/all/vault.yml` (chiffr√©), qui contient :

```yaml
gitlab_registry_url: "registry.gitlab.com"
gitlab_registry_user: "gregoire.elmacin"
gitlab_registry_token: "glpat-..."

```

J‚Äôai v√©rifi√© que le token √©tait bien disponible sur les h√¥tes avec :

```bash
ansible -i inventory/inventory.ini swarm_manager \
  -m debug -a "var=gitlab_registry_token" --ask-vault-pass

```

üëâ R√©sultat : les workers et le manager sont authentifi√©s proprement aupr√®s du registry GitLab, de fa√ßon d√©clarative.

---

## 3. On a fiabilis√© le Swarm c√¥t√© manager (r√¥le `swarm-manager`)

Avant, on faitais :

- `docker swarm init --advertise-addr ...` en `shell`
- puis `docker swarm join-token -q worker` et `manager` en `shell`
- avec de la logique manuelle pour v√©rifier si c‚Äô√©tait d√©j√† actif.

Maintenant, on fait √ßa en modules :

### 3.1. Initialisation idempotente du Swarm

```yaml
- name: Initialiser le Swarm en mode manager (idempotent)
  community.docker.docker_swarm:
    state: present
    advertise_addr: "{{ advertise_addr | default(ansible_default_ipv4.address) }}"
  register: swarm_init_result

```

üëâ Si le Swarm existe d√©j√†, la t√¢che est **idempotente**.

### 3.2. R√©cup√©ration des infos Swarm et des tokens

```yaml
- name: R√©cup√©rer les informations Swarm du manager
  community.docker.docker_swarm_info:
  register: swarm_info
  changed_when: false

```

Puis on v√©rifie via `docker info` que le Swarm est bien actif :

```yaml
- name: V√©rifier l'√©tat local du Swarm via docker info
  command: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
  register: manager_swarm_state
  changed_when: false

- name: √âchouer si le Swarm n'est pas actif sur le manager
  fail:
    msg: >
      Le Swarm n'est pas actif sur {{ inventory_hostname }}.
      √âtat retourn√© : {{ manager_swarm_state.stdout | default('N/A') | trim }}
  when: manager_swarm_state.stdout | default('') | trim != "active"

```

### 3.3. Exposition des tokens comme facts

Enfin, on expose les tokens et l‚ÄôIP du manager :

```yaml
- name: Exposer les tokens Swarm (worker & manager) comme facts
  set_fact:
    swarm_worker_token: "{{ swarm_info.swarm_facts.JoinTokens.Worker }}"
    swarm_manager_token: "{{ swarm_info.swarm_facts.JoinTokens.Manager }}"
    swarm_manager_ip: "{{ advertise_addr | default(ansible_default_ipv4.address) }}"

```

üëâ Ces facts sont ensuite utilis√©s par le r√¥le `swarm-worker` pour rejoindre proprement.

---

## 4. On a fiabilis√© le join des workers (r√¥le `swarm-worker`)

Avant, j‚Äôavais un gros bloc avec :

- `docker info` en shell,
- `docker swarm leave --force`,
- `docker swarm join --token ...` en shell,
- beaucoup de conditions pour tenter de nettoyer un ancien Swarm.

Maintenant j‚Äôutilise **`community.docker.docker_swarm`** c√¥t√© worker :

### 4.1. V√©rifier l‚Äô√©tat initial

```yaml
- name: V√©rifier l'√©tat local du Swarm
  command: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
  register: worker_swarm_state
  changed_when: false
  failed_when: false

```

### 4.2. Quitter un ancien Swarm si besoin

```yaml
- name: Quitter un Swarm existant si le n≈ìud est d√©j√† actif
  community.docker.docker_swarm:
    state: absent
    force: true
  when: worker_swarm_state.stdout | default('') | trim == "active"
  register: leave_result
  failed_when: false

```

### 4.3. Joindre le Swarm du manager avec le token expos√©

```yaml
- name: Joindre le Swarm manager avec community.docker.docker_swarm
  community.docker.docker_swarm:
    state: join
    advertise_addr: "{{ advertise_addr | default(ansible_default_ipv4.address) }}"
    join_token: "{{ swarm_worker_token }}"
    remote_addrs:
      - "{{ swarm_manager_ip }}:2377"
  register: join_result
  retries: 3
  delay: 5
  until: join_result is succeeded

```

### 4.4. V√©rification finale c√¥t√© worker

```yaml
- name: V√©rifier que le Swarm est actif c√¥t√© worker
  command: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
  register: worker_swarm_state_after
  changed_when: false

- name: √âchouer si le worker n'est pas dans le Swarm
  fail:
    msg: >
      Le worker {{ inventory_hostname }} n'a pas pu rejoindre le Swarm
      (√©tat final = {{ worker_swarm_state_after.stdout | default('N/A') | trim }}).
  when: worker_swarm_state_after.stdout | default('') | trim != "active"

```

üëâ R√©sultat : le join des workers est **d√©claratif, idempotent et robuste**.

---

## 5. On a mis les labels Swarm dans un r√¥le d√©di√© (`swarm-labels`)

Plut√¥t que d‚Äô√©parpiller la gestion des labels, j‚Äôai un r√¥le `swarm-labels` qui :

1. R√©cup√®re le `NodeID` de chaque n≈ìud :
    
    ```yaml
    - name: R√©cup√©rer l'ID du n≈ìud local dans le Swarm
      command: docker info --format '{{ "{{ .Swarm.NodeID }}" }}'
      register: swarm_node
      changed_when: false
    
    ```
    
2. Applique le label depuis le manager (avec `delegate_to`) :
    
    ```yaml
    - name: Ajouter le label role={{ swarm_role }} sur le n≈ìud (depuis le manager)
      command: docker node update --label-add role={{ swarm_role }} {{ swarm_node.stdout | trim }}
      delegate_to: "{{ groups['swarm_manager'][0] }}"
      when: swarm_role is defined
    
    ```
    

üëâ √áa nous permet, depuis l‚Äôinventaire, de d√©finir :

```
[swarm_managers]
master-vm swarm_role=manager ...

[swarm_app]
app-vm  swarm_role=app ...
app-vm2 swarm_role=app ...
app-vm3 swarm_role=app ...

[swarm_db]
db-vm   swarm_role=db ...

```

Et d‚Äôavoir des labels coh√©rents pour les contraintes Swarm.

---

## 6. On a professionnalis√© le d√©ploiement NocoDB (r√¥le `nocodb`)

### 6.1. Pull de l‚Äôimage priv√©e GitLab via `docker_image`

On utilise maintenant :

```yaml
- name: Pull l'image NocoDB depuis le registry GitLab (version fig√©e)
  community.docker.docker_image:
    name: "registry.gitlab.com/quickdata-batch22-group/nocodb/quickdata/nocodb:0.1.0"
    source: pull
    state: present
    force_source: true
  register: pull_result

```

L‚Äôauth est g√©r√©e **dans le r√¥le `docker`** via `docker_login`, donc on n‚Äôa plus besoin de `username/password` ici.

### 6.2. Templates de d√©ploiement

On g√©n√®re :

- le r√©pertoire cible :
    
    ```yaml
    file:
      path: /opt/nocodb
      state: directory
    
    ```
    
- le `compose.yml` :
    
    ```yaml
    template:
      src: compose.yml.j2
      dest: /opt/nocodb/compose.yml
    
    ```
    
- le `.env` :
    
    ```yaml
    template:
      src: .env.j2
      dest: /opt/nocodb/.env
    no_log: true
    
    ```
    

Les valeurs sensibles (mot de passe Postgres, NC_DB, etc.) viennent de mon vault.

### 6.3. D√©ploiement Swarm avec `-with-registry-auth`

C‚Äô√©tait un gros point : les workers refusaient de pull l‚Äôimage (erreurs `No such image`), car ils n‚Äôavaient pas les credentials du manager.

On a corrig√© le d√©ploiement :

```yaml
- name: D√©ployer la stack Swarm (avec credentials registry)
  command: >
    docker stack deploy
    --with-registry-auth
    -c /opt/nocodb/compose.yml
    mystack
  register: deploy_result
  changed_when: >
    'Updating service' in deploy_result.stdout or
    'Creating service' in deploy_result.stdout

```

üëâ Maintenant, les workers peuvent pull l‚Äôimage priv√©e GitLab.

### 6.4. V√©rification intelligente des r√©plicas NocoDB

Au lieu de faire un check instantan√©, on fait un **wait avec retry** :

```yaml
- name: Attendre que le service NocoDB ait au moins 1 r√©plique active
  command: docker service ls --format '{{ "{{ .Name }} {{ .Replicas }}" }}'
  register: services
  changed_when: false
  retries: 12
  delay: 5
  until: services.stdout is search('mystack_nocodb [1-9][0-9]*/[1-9][0-9]*')

- name: √âchouer si apr√®s attente NocoDB n'a toujours pas de r√©plique active
  fail:
    msg: >
      Le service NocoDB n'a pas de r√©plique active apr√®s timeout.
      docker service ls :
      {{ services.stdout_lines | default([]) }}
  when: services.stdout is not search('mystack_nocodb [1-9][0-9]*/[1-9][0-9]*')

```

üëâ Si `mystack_nocodb` finit √† `1/1`, Ansible est content.

üëâ Si on reste bloqu√© √† `0/1`, j‚Äôai un vrai signal de probl√®me applicatif.

---

## 7. On a ajout√© un playbook de nettoyage Swarm

En debug, j‚Äôai vu que mon cluster accumulait :

- des **nodes `Down`** (plusieurs IDs pour `app-vm`, `app-vm2`, `app-vm3`, `db-vm`),
- des services avec des r√©plicas bizarres (`mystack_root_db 5/1`, `mystack_nocodb 2/1`).

On a donc cr√©√© un **playbook sp√©cial nettoyage** (par exemple `playbooks/cleanup-swarm.yml`) qui :

1. Liste les n≈ìuds :
    
    ```yaml
    docker node ls --format '{{.ID}} {{.Hostname}} {{.Status}}'
    
    ```
    
2. Supprime les n≈ìuds en statut `Down` :
    
    ```yaml
    - name: Supprimer les n≈ìuds Swarm en statut Down
      command: docker node rm {{ item.split()[0] }}
      loop: "{{ swarm_nodes.stdout_lines }}"
      when: "'Down' in item"
      ignore_errors: true
    
    ```
    
3. Recale les services :
    
    ```yaml
    - name: S'assurer que les services mystack ont 1 r√©plique
      command: >
        docker service scale
        mystack_nocodb=1
        mystack_root_db=1
    
    ```
    

üëâ √áa permet de repartir sur un cluster plus propre (moins de fant√¥mes, r√©plicas align√©s avec la config).

---

## 8. Ce que j‚Äôai gagn√© avec cette √©volution

En r√©sum√©, avec cette v5 :

- On a une **architecture Ansible modulaire** bas√©e sur des r√¥les clairs.
- On utilise les **modules officiels `community.docker`** plut√¥t que des `shell` bruts.
- La gestion de Docker Swarm est **idempotente et fiable** :
    - init manager,
    - join des workers,
    - labels,
    - tokens.
- Le d√©ploiement NocoDB est :
    - versionn√© (`0.1.0`),
    - s√©curis√© (registry priv√© + vault),
    - supervis√© (check r√©plicas avec retry).
- On a des outils pour :
    - **d√©bugger** (debug facts, swarm_info),
    - **nettoyer** (playbook cleanup-swarm),
    - **rejouer** facilement (playbooks cibl√©s).

---
[Module suivant ‚Üí](M44_projet-board-J04.md)
---
