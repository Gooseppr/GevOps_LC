---
layout: page
title: Kubectl cmd
jour: 11|12
type: bonus
tags: docker, docker swarm, orchestration
---

# ğŸ³ **Module : Docker Swarm â€“ Orchestration distribuÃ©e**

*Orchestration simple, native Docker, idÃ©ale pour multi-VM en Ã©quipe*

---

# 1. ğŸ¯ **Objectifs du cours**

Ã€ la fin du module, tu seras capable de :

- Comprendre **pourquoi Docker Swarm existe** (vs Docker Compose + vs Kubernetes).
- Installer un **cluster Swarm complet** (manager + worker).
- DÃ©ployer une stack avec `docker stack deploy`.
- Utiliser les notions fondamentales :
    
    **Service, Task, Node, Stack, Overlay Network, Secrets, Replicas, Rolling Update**.
    
- Superviser et diagnostiquer un cluster.
- DÃ©ployer ton **projet NocoDB + monitoring + Postgres** sur EC2.

---

# 2. ğŸ§  **Introduction â€” Pourquoi Docker Swarm ?**

Docker Compose te permet de gÃ©rer plusieurs conteneursâ€¦ mais **seulement sur une machine** (orchestrateur local)

Docker lui-mÃªme ne sait **pas** :

- rÃ©partir des conteneurs sur plusieurs serveurs,
- maintenir un nombre de rÃ©plicas,
- survivre Ã  la panne dâ€™une machine,
- mettre Ã  jour sans downtime,
- crÃ©er un rÃ©seau distribuÃ© sÃ©curisÃ©.

ğŸ‘‰ Câ€™est exactement lÃ  quâ€™intervient **Docker Swarm**.

---

# 3. ğŸ **Quâ€™est-ce que Docker Swarm ?**

Docker Swarm = le *mode cluster* de Docker.

Câ€™est un orchestrateur simple, intÃ©grÃ©, prÃªt Ã  lâ€™emploi.

Il ajoute Ã  Docker les super-pouvoirs :

| CapacitÃ© | Description |
| --- | --- |
| **Multi-server** | Plusieurs machines = 1 cluster |
| **Services & replicas** | Auto-scaling, tolÃ©rance aux pannes |
| **Rolling updates** | Mises Ã  jour progressives sans coupure |
| **RÃ©seaux overlay** | Communication sÃ©curisÃ©e inter-VM |
| **Secrets** | Stockage sÃ©curisÃ© des mots de passe |
| **Scheduler** | Placement intelligent des conteneurs |

Swarm est souvent vu comme :

> ğŸ”¥ â€œLe Kubernetes simpleâ€
> 
> 
> (moins puissant, mais plus rapide Ã  apprendre et parfait pour les projets dâ€™Ã©cole, petites Ã©quipes, PoC ou projets internes)
> 

---

# 4. ğŸ—ï¸ **Architecture Docker Swarm**

Voici un schÃ©ma clair :

```mermaid
flowchart LR
    subgraph Manager_1["Manager Node"]
        A1["Swarm Manager<br/>scheduler, control plane"]
        A2["Services Definition"]
    end

    subgraph Worker_1["Worker Node"]
        B1["Task 1<br/>(NocoDB)"]
        B2["Task 2<br/>(Node Exporter)"]
    end

    subgraph Worker_2["Worker Node"]
        C1["Task 3<br/>(Promtail)"]
    end

    Manager_1 -->|contrÃ´le| Worker_1
    Manager_1 -->|contrÃ´le| Worker_2
    Worker_1 <-->|overlay network| Worker_2

```

### Les rÃ´les essentiels :

### ğŸŸ¢ **Manager node**

- orchestre le cluster
- prend les dÃ©cisions de placement
- maintient lâ€™Ã©tat dÃ©sirÃ©
- stocke la configuration du Swarm

### ğŸ”µ **Worker node**

- exÃ©cute les tÃ¢ches (tasks)
- reÃ§oit des instructions du manager

### ğŸ”¶ **Service**

La dÃ©finition logique (comme dans Compose).

### ğŸ”· **Task**

Lâ€™instance dâ€™un conteneur, exÃ©cutÃ©e sur un Worker.

---

# 5. âš™ï¸ **Installation de Docker Swarm**

Avant tout : **Docker doit Ãªtre installÃ© sur toutes les VM** (rÃ©fÃ©rence installation Docker â†’ module Docker de ton cours ).

Ensuite, tu vas crÃ©er :

- 1 VM **manager**
- 1 ou plusieurs VM **workers**

## 5.1 Initialiser le cluster (sur le manager)

```bash
docker swarm init --advertise-addr <IP_publique_du_manager>

```

RÃ©sultat :

```
Swarm initialized: current node (xxxxx) is now a manager.

docker swarm join --token SWMTKN-xxxxx <IP_manager>:2377

```

Ce token est **la clÃ© secrÃ¨te** nÃ©cessaire pour rejoindre le cluster.

## 5.2 Ajouter un node worker

Sur chaque worker :

```bash
docker swarm join --token SWMTKN-xxxxx <IP_manager>:2377

```

VÃ©rification (sur le manager) :

```bash
docker node ls

```

---

# 6. ğŸ”— **Concepts fondamentaux Ã  maÃ®triser**

## 6.1 Service

Equivalent Ã  `docker run`, mais distribuÃ©.

Exemple :

```bash
docker service create --name web --replicas 3 nginx

```

## 6.2 Replicas

Nombre dâ€™instances dÃ©sirÃ©es :

```bash
docker service scale web=5

```

## 6.3 Task

Une instance â€œvivanteâ€ dâ€™un service sur un Worker.

## 6.4 Overlay networks

RÃ©seau inter-VM sÃ©curisÃ© pour que les services communiquent.

```bash
docker network create -d overlay mynet

```

## 6.5 Stack (fichier Compose version Swarm)

Equivalent Ã  un `docker-compose.yml` mais dÃ©ployÃ© sur plusieurs machines :

```bash
docker stack deploy -c docker-compose.yml mystack

```

## 6.6 Secrets

Pour stocker des mots de passe :

```bash
echo "password123" | docker secret create db_password -

```

---

# 7. ğŸ“¦ **CrÃ©er ton premier Swarm Service**

### Nginx avec 3 rÃ©plicas :

```bash
docker service create \
  --name nginx \
  --replicas 3 \
  --publish 8080:80 \
  nginx

```

### VÃ©rification :

```bash
docker service ls         # liste des services
docker service ps nginx   # oÃ¹ sont placÃ©es les tasks

```

---

# 8. ğŸ§° **DÃ©ployer une Stack avec docker-compose.yml**

Docker Swarm utilise un fichier **Compose**, mais avec des options spÃ©cifiques.

Exemple complet :

```yaml
version: "3.9"

services:
  web:
    image: nginx
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 5s
      restart_policy:
        condition: on-failure
    networks:
      - frontend

networks:
  frontend:
    driver: overlay

```

DÃ©ploiement :

```bash
docker stack deploy -c docker-compose.yml mystack

```

Listage :

```bash
docker stack ls
docker stack services mystack
docker stack ps mystack

```

---

# 9. ğŸ§± **Les options essentielles (bien expliquÃ©es)**

## 9.1 deploy.update_config

ContrÃ´le le **rolling update** :

```yaml
update_config:
  parallelism: 1   # 1 conteneur mis Ã  jour Ã  la fois
  delay: 5s        # pause entre chaque update

```

## 9.2 deploy.restart_policy

ContrÃ´le la maniÃ¨re dont un service redÃ©marre :

- `none`
- `on-failure`
- `any`

## 9.3 deploy.placement

ContrÃ´le oÃ¹ les conteneurs sâ€™exÃ©cutent.

Exemples :

### ExÃ©cuter seulement sur un worker :

```yaml
placement:
  constraints:
    - node.role == worker

```

### Label personnalisÃ© :

```bash
docker node update --label-add type=db node-2

```

YAML :

```yaml
placement:
  constraints:
    - node.labels.type == db

```

---

# 10. ğŸ” **Docker Secrets â€” indispensable en production**

CrÃ©er un secret :

```bash
echo "mypassword" | docker secret create db_pass -

```

Le rÃ©fÃ©rencer :

```yaml
services:
  db:
    image: postgres
    secrets:
      - db_pass

secrets:
  db_pass:
    external: true

```

Dans le conteneur, le secret apparaÃ®t dans :

```
/run/secrets/db_pass

```

---

# 11. ğŸ›¡ï¸ **RÃ©seaux Overlay â€“ le cÅ“ur du Swarm**

CrÃ©e un rÃ©seau inter-VM :

```bash
docker network create -d overlay --attachable monitoring

```

Attachable = permet aux conteneurs non-stack de rejoindre le rÃ©seau (utile pour debug).

---

# 12. ğŸ“Š **Monitoring du cluster**

### Liste des nodes :

```bash
docker node ls

```

### Status dÃ©taillÃ© :

```bash
docker node inspect node-1 --pretty

```

### Journaux :

```bash
docker service logs -f nginx

```

---

# 13. ğŸ§ª **Tester ton cluster Swarm (TP rapide)**

## 13.1 Test de tolÃ©rance aux pannes

1. DÃ©ploie Nginx avec 3 rÃ©plicas.
2. Coupe un worker :

```bash
sudo shutdown -h now

```

1. Observe :

```bash
docker service ps nginx
docker node ls

```

â¡ï¸ Swarm recalcule automatiquement lâ€™Ã©tat dÃ©sirÃ© et redÃ©ploie ailleurs.

---

# 14. ğŸ­ **DÃ©ployer ton projet NocoDB sur Swarm (exemple complet)**

Voici une stack adaptÃ©e Ã  ton projet :

```yaml
version: "3.9"

services:
  nocodb:
    image: nocodb/nocodb:latest
    ports:
      - "8080:8080"
    environment:
      NC_DB: "pg://root_db:5432?u=postgres&p=password&d=root_db"
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 5s
    networks:
      - internal
      - ingress

  promtail:
    image: grafana/promtail
    deploy:
      replicas: 1
    networks:
      - internal

  nodeexporter:
    image: prom/node-exporter
    deploy:
      mode: global
    networks:
      - internal

networks:
  internal:
    driver: overlay
  ingress:
    driver: overlay

```

DÃ©ploiement :

```bash
docker stack deploy -c swarm.yml nocostack

```

---

# 15. ğŸ¨ **Mindmap rÃ©capitulative**

```mermaid
mindmap
  root((Docker Swarm))
    Architecture
      Manager
      Workers
      Overlay Network
    Services
      Replicas
      Tasks
      Rolling Updates
      Placement
    Stacks
      docker stack deploy
      docker stack ps
    SÃ©curitÃ©
      Secrets
      Chiffrement networks
    Use cases
      Multi-VM
      Monitoring
      Microservices

```

---

# 16. ğŸ§  **RÃ©sumÃ© gÃ©nÃ©ral (style formateur)**

- Docker Swarm = **orchestrateur multi-serveurs natif Docker**, plus simple que Kubernetes.
- Il introduit : **services**, **rÃ©plicas**, **rolling updates**, **overlay networks**, **secrets**.
- Il se dÃ©ploie en **10 minutes** contre plusieurs heures pour Kubernetes.
- Parfait pour ton stack AWS :
    
    **1 VM DB + 1 VM Swarm Manager + 1 Worker**.

# 17. ğŸ”§ Commandes utiles pour dÃ©bugger Docker Swarm

## 1ï¸âƒ£ VÃ©rifier lâ€™Ã©tat gÃ©nÃ©ral du cluster

### Infos globales

```bash
docker info

```

âœ Te dit si le nÅ“ud est `Swarm: active` ou `inactive`, et sâ€™il est `Manager` ou `Worker`.

---

### Liste des nÅ“uds du Swarm

```bash
docker node ls

```

âœ Indispensable pour voir :

- Quel nÅ“ud est `Leader`
- Quels nÅ“uds sont `Ready` / `Down`
- Lâ€™`Availability` : `Active`, `Drain`â€¦

---

### DÃ©tails sur un nÅ“ud

```bash
docker node inspect <node-id-ou-hostname> --pretty

```

Exemple :

```bash
docker node inspect app-vm --pretty
docker node inspect db-vm --pretty

```

âœ Te permet de vÃ©rifier :

- les labels (`Labels:`),
- le rÃ´le (manager/worker),
- lâ€™Ã©tat (`Status:`),
- lâ€™IP utilisÃ©e dans le Swarm.

---

## 2ï¸âƒ£ Debug des services et tasks

### Liste de tous les services

```bash
docker service ls

```

âœ Pour voir rapidement :

- combien de services,
- combien de replicas (ex : `1/1`, `3/3`, `0/1`â€¦).

---

### Voir les tasks dâ€™un service (avec les erreurs)

```bash
docker service ps <service>
docker service ps <service> --no-trunc

```

Exemples :

```bash
docker service ps mystack_nocodb
docker service ps mystack_root_db --no-trunc

```

âœ Super utile pour voir :

- sur **quel nÅ“ud** chaque task tourne,
- `CURRENT STATE` (Running, Failed, Rejectedâ€¦),
- la colonne `ERROR` quand un conteneur nâ€™arrive pas Ã  dÃ©marrer.

---

### Inspecter la spec complÃ¨te dâ€™un service

```bash
docker service inspect mystack_nocodb --pretty
docker service inspect mystack_nocodb --format '{{json .Spec.TaskTemplate.ContainerSpec.Env}}'

```

âœ Tu lâ€™utilises dÃ©jÃ  pour vÃ©rifier les **variables dâ€™environnement** propagÃ©es par Swarm (`NC_DB`, `POSTGRES_*`, etc.).

---

### Logs dâ€™un service

```bash
docker service logs mystack_nocodb
docker service logs mystack_nocodb --tail 100
docker service logs mystack_nocodb -f

```

âœ Câ€™est ce qui tâ€™a donnÃ© lâ€™erreur :

> getaddrinfo ENOTFOUND root_db
> 

et permet de confirmer :

- problÃ¨me de **rÃ©solution DNS**,
- problÃ¨me de **connexion DB**,
- crash loop.

---

## 3ï¸âƒ£ Debug des stacks

### Liste des stacks

```bash
docker stack ls

```

---

### Services dans une stack

```bash
docker stack services mystack

```

âœ Vue rapide : `REPLICAS`, `IMAGE`, `PORTS`.

---

### Tasks de tous les services dâ€™une stack

```bash
docker stack ps mystack
docker stack ps mystack --no-trunc

```

âœ Vue globale du Swarm pour cette stack : oÃ¹ tournent les tasks, Ã©tat global.

---

### Re-dÃ©ployer une stack

```bash
docker stack deploy -c compose.yml mystack

```

Tu lâ€™utilises dÃ©jÃ  dans Ansible :

```yaml
- name: DÃ©ployer la stack Docker Swarm
  shell: docker stack deploy -c {{ deploy_path }}/compose.yml {{ stack_name }}
  args:
    chdir: "{{ deploy_path }}"

```

---

## 4ï¸âƒ£ Debug par nÅ“ud (tasks sur un node spÃ©cifique)

### Voir ce qui tourne sur un nÅ“ud

```bash
docker node ps app-vm
docker node ps db-vm

```

âœ TrÃ¨s pratique quand tu as plusieurs workers app (app-vm, app-replica, app-replica2â€¦) pour voir **quels services sont planifiÃ©s oÃ¹**.

---

## 5ï¸âƒ£ RÃ©seau & DNS dans Swarm

Tu as dÃ©jÃ  utilisÃ© ce pattern, câ€™est un des meilleurs outils de debug rÃ©seau dans Swarm ğŸ‘‡

### Lister les networks

```bash
docker network ls

```

---

### Inspecter un network overlay

```bash
docker network inspect mystack_backend
docker network inspect mystack_default

```

âœ Pour voir :

- quels services / conteneurs sont connectÃ©s,
- lâ€™`IPAM`,
- les nÅ“uds oÃ¹ le rÃ©seau existe.

---

### Tester la rÃ©solution DNS dans le rÃ©seau de la stack

```bash
docker run --rm --network mystack_backend alpine nslookup root_db
docker run --rm --network mystack_backend alpine ping -c 3 root_db

```

âœ Câ€™est ce qui tâ€™aurait permis de vÃ©rifier si `root_db` est **rÃ©soluble** depuis le rÃ©seau overlay et dâ€™attraper ton bug plus vite.

---

## 6ï¸âƒ£ Debug conteneur â€œclassiqueâ€ (si besoin de zoomer)

MÃªme si on est dans Swarm, parfois tu veux descendre au niveau **conteneur**.

### Voir les conteneurs sur une VM

```bash
docker ps
docker ps -a

```

---

### Inspecter & log dâ€™un conteneur particulier

```bash
docker logs <container-id>
docker logs -f <container-id>

docker inspect <container-id>

```

---

### Entrer dans un conteneur

```bash
docker exec -it <container-id> sh
# ou bash selon l'image

```

---

## 7ï¸âƒ£ Diagnostic Docker / systÃ¨me

Quand tu suspectes un souci plus bas niveau :

```bash
systemctl status docker
journalctl -u docker --since "10 minutes ago"
docker events --since 10m

```

---

## 8ï¸âƒ£ RÃ©sumÃ© â€œkit de baseâ€

Pour ton projet actuel (NocoDB + Postgres sur Swarm), les **commandes que tu vas utiliser tout le temps** :

```bash
# Vue cluster
docker info
docker node ls
docker node inspect app-vm --pretty

# Vue stack & services
docker stack ls
docker stack services mystack
docker stack ps mystack

docker service ls
docker service ps mystack_nocodb --no-trunc
docker service logs mystack_nocodb --tail 50

# Vue par node
docker node ps app-vm
docker node ps db-vm

# RÃ©seau & DNS
docker network ls
docker network inspect mystack_backend
docker run --rm --network mystack_backend alpine nslookup root_db

```