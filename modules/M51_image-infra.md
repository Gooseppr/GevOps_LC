---
title: Projet J11
sujet: Projet NocoDb NocoDb
type: module
jour: 51
ordre: 0
tags: projet, board
---

# √âtat de l‚Äôinfrastructure DevOps ‚Äì 18 d√©cembre

## 1. Objectif de l‚Äôinfrastructure au 18/12

√Ä cette date, l‚Äôobjectif est d‚Äôavoir une **infrastructure AWS compl√®te, reproductible et exploitable**, permettant :

- le d√©ploiement d‚Äôune application NocoDB en environnement distribu√©,
- l‚Äôorchestration via Docker Swarm,
- une exposition s√©curis√©e via Traefik,
- une s√©paration claire des r√¥les r√©seau et applicatifs,
- un syst√®me de sauvegarde PostgreSQL externalis√© sur S3,
- et une base d‚Äôobservabilit√© minimale.

L‚Äôinfrastructure est d√©sormais **fonctionnelle, test√©e et stabilis√©e**.

---

## 2. D√©coupage Terraform actuel

L‚Äôinfrastructure est volontairement d√©coup√©e en **deux states Terraform distincts**, chacun ayant une responsabilit√© claire.

### 2.1 Terraform ‚Äì Infra applicative (`terraform/infra`)

Ce state est responsable de tout ce qui est **volatile / reconstructible** :

- R√©seau AWS (VPC, subnets, routes, NAT, IGW)
- Machines virtuelles
- Security Groups
- Cl√©s SSH
- R√¥les IAM li√©s √† l‚Äôexploitation
- Inventaire Ansible

üëâ Ce state peut √™tre **d√©truit et recr√©√©** sans perte de donn√©es critiques.

### 2.2 Terraform ‚Äì Stockage S3 (`terraform/S3`)

Ce state est responsable **uniquement du stockage de sauvegarde** :

- Bucket S3 PostgreSQL
- Blocage total de l‚Äôacc√®s public
- Politique de r√©tention automatique
- Outputs exposant le nom du bucket

üëâ Ce state est **d√©ploy√© √† part** et consid√©r√© comme **long-lived**.

---

## 3. Infrastructure r√©seau AWS (photo au 18/12)

### 3.1 VPC et segmentation r√©seau

L‚Äôinfrastructure repose sur un VPC unique avec plusieurs subnets fonctionnels :

- subnet bastion
- subnet gateway / traefik
- subnet application
- subnet database
- subnet monitoring

Un **NAT d√©di√©** permet la sortie internet des subnets priv√©s.

### Sch√©ma r√©seau

```mermaid
flowchart LR
Internet((Internet)) --> IGW[Internet Gateway]

IGW --> Bastion[Bastion]
IGW --> Traefik[Traefik<br/>Swarm Manager]

subgraph VPC[VPC nocodb_vpc]
NAT[NAT<br/>fck-nat]
App1[application-1]
App2[application-2]
DB[database-1]
Mon[monitoring]
end

App1 -->|egress| NAT
App2 -->|egress| NAT
DB -->|egress| NAT
Mon -->|egress| NAT

```

üëâ Cette architecture permet :

- un point d‚Äôentr√©e ma√Ætris√©,
- une s√©paration des r√¥les,
- une sortie internet contr√¥l√©e pour les services internes.

---

## 4. Machines virtuelles et r√¥les

Terraform provisionne les instances suivantes :

| VM | R√¥le |
| --- | --- |
| bastion | Acc√®s administratif |
| traefik | Swarm manager + reverse proxy |
| application-1 / application-2 | Services applicatifs |
| database-1 | PostgreSQL |
| monitoring | Supervision |
| nat | Sortie internet priv√©e |

Chaque VM dispose :

- d‚Äôun **Security Group d√©di√©**,
- d‚Äôune **cl√© SSH g√©n√©r√©e automatiquement**,
- d‚Äôun nom coh√©rent avec l‚Äôinventaire Ansible.

---

## 5. Docker Swarm (orchestration)

### 5.1 Organisation du cluster

- `traefik` : **manager Swarm**
- `application-*`, `database-1`, `monitoring` : **workers**

Des **labels Swarm** sont appliqu√©s automatiquement :

- `role=infra`
- `role=application`
- `role=database`
- `role=monitoring_group`

üëâ Ces labels permettent un **placement strict** des services.

### Sch√©ma de placement Swarm

```mermaid
flowchart LR
  Traefik["Node: traefik\nrole=infra"] --> RP["Reverse Proxy\n(Traefik)"]

  DBN["Node: database-1\nrole=database"] --> PG["PostgreSQL"]

  AppN["Nodes: application-1/2\nrole=application"] --> APP["Services applicatifs"]

  MonN["Node: monitoring\nrole=monitoring_group"] --> OBS["Monitoring"]

```

---

## 6. D√©ploiement applicatif

### 6.1 Reverse proxy

- Traefik d√©ploy√© via `docker stack`
- TLS activ√©
- Routage centralis√©

### 6.2 Stack NocoDB

- Image NocoDB versionn√©e depuis GitLab Registry
- PostgreSQL d√©ploy√© sur le n≈ìud `database`
- R√©seaux overlay `backend` et `proxy`
- Attente active sur la r√©plication Swarm

üëâ Le d√©ploiement est **idempotent** et tol√©rant aux d√©lais Swarm.

---

## 7. Sauvegarde PostgreSQL ‚Äì √©tat final

### 7.1 Stockage S3

Le bucket est d√©sormais :

- cr√©√© via `terraform/S3`,
- nomm√© dynamiquement avec suffixe utilisateur,
- prot√©g√© contre l‚Äôacc√®s public,
- dot√© d‚Äôune politique de r√©tention.

Exemple r√©el au 18/12 :

```bash
export TF_VAR_bucket_suffix=grego
tofu apply -auto-approve

```

R√©sultat :

```
quickdata-pg-backups-storage-bucket-grego

```

### 7.2 Logique DevOps

- Le **bucket n‚Äôest pas d√©truit** lors d‚Äôun `destroy` de l‚Äôinfra.
- Les backups survivent aux reconstructions compl√®tes.
- Le nom du bucket est inject√© dans Ansible via outputs / remote state.

üëâ Le stockage devient une **brique ind√©pendante**, comme en production.

---

## 8. Cha√Æne d‚Äôexploitation valid√©e

```mermaid
flowchart TB
  TF_S3["Terraform S3\n(bucket + lifecycle)"]
  TF_INFRA["Terraform Infra\n(VPC, VM, IAM, inventory)"]

  TF_INFRA --> ANS["Ansible\nSwarm + Traefik + NocoDB + Postgres"]
  TF_S3 --> ANS

```

---

## 9. Conclusion (photo au 18/12)

√Ä la date du 18 d√©cembre, l‚Äôinfrastructure pr√©sente :

- une s√©paration claire **infra / stockage**,
- une orchestration Swarm fonctionnelle,
- un d√©ploiement applicatif stable,
- une gestion s√©curis√©e des acc√®s,
- une approche DevOps r√©aliste et industrialisable.

Cette version constitue une **base solide**, proche d‚Äôune architecture de production, sur laquelle peuvent d√©sormais s‚Äôajouter :

- la restauration automatis√©e,
- la supervision avanc√©e,
- ou des pipelines CI/CD.

---
[‚Üê Module pr√©c√©dent](M51_backup-S3-terra.md) | [Module suivant ‚Üí](M51_projet-board-J11.md)
---
