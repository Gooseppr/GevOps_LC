---
title: Projet J1
sujet: Projet
type: module
jour: 41
ordre: 2
tags: projet, docker, docker swarm, stack, ansible
---

# üöÄ R√©capitulatif complet : Automatisation du d√©ploiement NocoDB + Postgres via Docker Swarm + Ansible

---

# 1Ô∏è‚É£ Objectif du projet

Automatiser enti√®rement un d√©ploiement de production :

- 2 VMs sur AWS (manager + worker)
- Docker Swarm
- Un r√©seau overlay
- Un Postgres d√©ploy√© sur le n≈ìud `db`
- Un NocoDB d√©ploy√© sur le n≈ìud `app`
- Un syst√®me Ansible qui configure, installe, rejoint le cluster et d√©ploie la stack automatiquement

---

# 2Ô∏è‚É£ Les probl√®mes rencontr√©s et comment tu les as r√©solus

Je te d√©taille **tous les blocages**, avec **ce qui n'allait pas**, **comment tu l‚Äôas compris**, et **la solution que tu as mise en place**.

---

## üß© Probl√®me 1 ‚Äî Le worker ne rejoignait pas le Swarm

### ‚ùå Sympt√¥mes

- `db-vm` avait plusieurs NodeID
- Le Swarm affichait plusieurs entr√©es ‚Äúdb-vm‚Äù, certaines en ‚ÄúDown‚Äù
- Le label ne s‚Äôappliquait pas

### üìå Cause

Le worker rejoignait parfois, mais pas avec la bonne IP, ou alors il ne quittait pas correctement un ancien Swarm.

### ‚úÖ Solution mise en place

Tu as renforc√© l'√©tape Ansible de join :

- quitter proprement un Swarm existant
- v√©rifier l‚Äô√©tat avant/apr√®s
- attendre que le n≈ìud soit **Ready Active** c√¥t√© manager
- fail si quelque chose ne va pas

üëâ R√©sultat : join fiable √† 100%.

---

## üß© Probl√®me 2 ‚Äî Les labels Swarm ne s‚Äôappliquaient pas

### ‚ùå Sympt√¥me

Le r√¥le `app` et `db` n‚Äô√©tait pas appliqu√© sur tous les n≈ìuds.

### üìå Cause

La t√¢che Ansible utilisait une mauvaise logique (run_once, mauvaise r√©cup√©ration NodeID).

### ‚úÖ Solution

Tu as corrig√© :

- r√©cup√©ration r√©elle du NodeID via `docker info`
- fail explicite si NodeID absent
- `delegate_to: manager` pour appliquer les labels depuis le bon n≈ìud
- pas de `run_once`: ex√©cution pour chaque n≈ìud

üëâ R√©sultat :

`app-vm` a bien `role=app`

`db-vm` a bien `role=db`

Et les placements fonctionnent üéØ

---

## üß© Probl√®me 3 ‚Äî Postgres ne d√©marrait pas

### ‚ùå Sympt√¥mes

- `postgres:16` en boucle FAILED
- code exit(1)
- pas plus de logs

### üìå Cause

Le conteneur n‚Äôavait **pas acc√®s √† un r√©seau overlay valide** ‚Üí impossible de r√©soudre son propre hostname.

### ‚úÖ Solution

Tu as ajout√© :

```yaml
networks:
  backend:
    driver: overlay

```

Et dans chaque service :

```yaml
networks:
  - backend

```

üëâ R√©sultat :

`mystack_backend` cr√©√© automatiquement ‚Üí Postgres d√©marre.

---

## üß© Probl√®me 4 ‚Äî NocoDB ne trouvait pas Postgres (ENOTFOUND root_db)

### ‚ùå Sympt√¥mes

```
Error: getaddrinfo ENOTFOUND root_db

```

### üìå Cause

- Soit Postgres ne d√©marrait pas (probl√®me 3)
- Soit la variable `NC_DB` √©tait mal construite
- Soit le r√©seau overlay n‚Äôexistait pas

### ‚úÖ Solution

Tu as corrig√© les variables :

### Avant (mauvais) :

```
NC_DB=pg://:5432?u=&p=&d=

```

### Apr√®s (bon) :

```
POSTGRES_HOST=root_db
POSTGRES_DB=nocodb_db
POSTGRES_USER=admin
POSTGRES_PASSWORD=passwordS√©cur1t√©
NC_DB=pg://root_db:5432?u=admin&p=passwordS√©cur1t√©&d=nocodb_db

```

üëâ R√©sultat :

NocoDB d√©marre et se connecte √† Postgres comme pr√©vu üéâ

---

## üß© Probl√®me 5 ‚Äî Ansible √©chouait lors du `docker stack deploy`

### ‚ùå Sympt√¥me

```
service root_db: undefined network "backend"

```

### üìå Cause

Le r√©seau overlay n‚Äô√©tait pas d√©clar√©.

### ‚úÖ Solution

Tu as ajout√© :

```yaml
networks:
  backend:
    driver: overlay

```

üëâ R√©sultat : le `stack deploy` fonctionne sans erreur.

---

# 3Ô∏è‚É£ Ajustements du plan initial avant d‚Äôajouter les workers

Avant de passer √† l‚Äô√©tape ¬´ 3. Joindre les workers au Swarm ¬ª, j‚Äôai d√ª adapter plusieurs √©l√©ments de mon plan de d√©part pour que le d√©ploiement soit r√©ellement **reproductible** et **fiable** avec Ansible.

## Inventaire Ansible : s√©paration manager / worker + cl√© SSH

Dans `inventory.ini`, j‚Äôai :

```
[swarm_managers]
app-vm ansible_host=35.181.155.123 ansible_user=admin swarm_role=app ansible_ssh_private_key_file=/workspaces/dev-env/key/web-server.pem

[swarm_workers]
db-vm ansible_host=15.188.14.104 ansible_user=admin swarm_role=db ansible_ssh_private_key_file=/workspaces/dev-env/key/web-server.pem

[swarm_all:children]
swarm_managers
swarm_workers

```

**Pourquoi :**

- Je distingue clairement :
    - `swarm_managers` ‚Üí n≈ìud qui initialise le Swarm et d√©ploie la stack.
    - `swarm_workers` ‚Üí n≈ìuds qui rejoignent le Swarm et h√©bergent des services.
- J‚Äôajoute une variable `swarm_role` (`app` ou `db`) qui sera utilis√©e plus tard pour appliquer les labels Swarm.
- J‚Äôutilise une **cl√© SSH** commune (`web-server.pem`) avec `ansible_ssh_private_key_file` pour √©viter tout prompt de mot de passe.

C√¥t√© machine locale, j‚Äôai aussi ajout√© les h√¥tes dans `known_hosts` pour qu‚ÄôAnsible ne bloque pas sur la demande de confirmation :

```bash
ssh-keyscan 35.181.155.123 >> ~/.ssh/known_hosts   # app-vm
ssh-keyscan 15.188.14.104 >> ~/.ssh/known_hosts   # db-vm

```

---

## Initialisation du Swarm : IP priv√©e + stockage dans les facts

Dans le play `2. Initialiser le Swarm sur le manager` (`swarm.yml`), j‚Äôai renforc√© la logique :

```yaml
- name: 2. Initialiser le Swarm sur le manager
  hosts: swarm_managers
  become: true
  vars:
    # IP priv√©e du manager (celle utilis√©e pour --advertise-addr)
    manager_ip: "{{ ansible_default_ipv4.address }}"
  tasks:
    - name: V√©rifier l'√©tat local du Swarm
      shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
      register: swarm_state
      changed_when: false
      failed_when: false

    - name: Initialiser le Swarm (si pas encore actif)
      shell: docker swarm init --advertise-addr {{ manager_ip }}
      when: swarm_state.stdout is not defined or swarm_state.stdout | trim != "active"

    - name: V√©rifier que le Swarm est bien actif (s√©curit√©)
      shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
      register: swarm_state_after
      changed_when: false

    - name: √âchouer si le Swarm n'est pas actif sur le manager
      fail:
        msg: "Le Swarm n'est pas actif sur le manager (√©tat={{ swarm_state_after.stdout | trim }})."
      when: swarm_state_after.stdout | trim != "active"

    - name: R√©cup√©rer le token worker
      shell: docker swarm join-token -q worker
      register: worker_token

    - name: V√©rifier que le token worker n'est pas vide
      fail:
        msg: "Token worker vide : impossible de joindre les workers au Swarm."
      when: worker_token.stdout is not defined or worker_token.stdout | length == 0

    - name: R√©cup√©rer le token manager
      shell: docker swarm join-token -q manager
      register: manager_token

    - name: Sauver les tokens et l'IP du manager dans les hostvars
      set_fact:
        swarm_worker_token: "{{ worker_token.stdout }}"
        swarm_manager_token: "{{ manager_token.stdout }}"
        swarm_manager_ip: "{{ manager_ip }}"

```

**Ce que j‚Äôai chang√© par rapport √† mon id√©e initiale :**

1. **Utilisation de l‚ÄôIP priv√©e (`ansible_default_ipv4.address`)**
    
    Sur AWS, le Swarm doit communiquer via les IP priv√©es, pas via les IP publiques.
    
    ‚Üí Je force `--advertise-addr` √† l‚ÄôIP priv√©e du manager.
    
2. **V√©rifications explicites de l‚Äô√©tat Swarm**
    - Je v√©rifie l‚Äô√©tat avant (`swarm_state`) et apr√®s (`swarm_state_after`) l‚Äôinit.
    - Si l‚Äô√©tat n‚Äôest pas `active`, je fais **√©chouer le play** tout de suite.
        
        ‚Üí √áa √©vite de continuer avec un Swarm √† moiti√© initialis√©.
        
3. **Stockage des tokens et de l‚ÄôIP dans les facts (`set_fact`)**
    - `swarm_worker_token` et `swarm_manager_token`
    - `swarm_manager_ip`
        
        Ces valeurs sont ensuite r√©utilis√©es dans le play suivant pour joindre les workers, au lieu de recalculer ou hardcoder quoi que ce soit.
        

---

## Joindre les workers : logique robuste plut√¥t qu‚Äôun simple join

Dans le play `3. Joindre les workers au Swarm`, j‚Äôai d√ª aller beaucoup plus loin que mon id√©e de base ‚Äúfaire un simple `docker swarm join`‚Äù :

```yaml
- name: 3. Joindre les workers au Swarm
  hosts: swarm_workers
  become: true
  vars:
    manager_name: "{{ groups['swarm_managers'][0] }}"
    # On r√©utilise l'IP priv√©e du manager enregistr√©e √† l'√©tape 2
    manager_ip: "{{ hostvars[manager_name].swarm_manager_ip | default(hostvars[manager_name].ansible_default_ipv4.address) }}"
    worker_token: "{{ hostvars[manager_name].swarm_worker_token }}"
  tasks:
    - name: V√©rifier l'√©tat local du Swarm sur le worker
      shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
      register: worker_swarm_state
      changed_when: false
      failed_when: false

    - name: Quitter un Swarm existant sur le worker (si d√©j√† actif)
      shell: docker swarm leave --force
      when: worker_swarm_state.stdout is defined and worker_swarm_state.stdout | trim == "active"

    - name: Rafra√Æchir l'√©tat local du Swarm apr√®s √©ventuel leave
      shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
      register: worker_swarm_state_after
      changed_when: false
      failed_when: false

    - name: Joindre le Swarm en tant que worker
      shell: docker swarm join --token {{ worker_token }} {{ manager_ip }}:2377
      register: join_result
      when: worker_swarm_state_after.stdout is not defined or worker_swarm_state_after.stdout | trim != "active"
      retries: 5
      delay: 5
      until: join_result.rc == 0 or 'already part of a swarm' in join_result.stderr
      failed_when: >
        join_result.rc != 0 and
        'already part of a swarm' not in join_result.stderr

    - name: V√©rifier c√¥t√© worker que le Swarm est actif apr√®s join
      shell: docker info --format '{{ "{{ .Swarm.LocalNodeState }}" }}'
      register: worker_swarm_state_final
      changed_when: false

    - name: √âchouer si le Swarm n'est toujours pas actif sur le worker
      fail:
        msg: "Le Swarm n'est pas actif sur le worker {{ inventory_hostname }} (√©tat={{ worker_swarm_state_final.stdout | trim }})."
      when: worker_swarm_state_final.stdout | trim != "active"

    - name: Attendre que le n≈ìud soit visible c√¥t√© manager et Ready/Active
      delegate_to: "{{ manager_name }}"
      shell: docker node ls --format '{{ "{{ .Hostname }} {{ .Status }} {{ .Availability }}" }}'
      register: node_list
      retries: 12
      delay: 5
      until: node_list.stdout is search(inventory_hostname + " Ready Active")
      changed_when: false
      failed_when: node_list.stdout is not search(inventory_hostname + " Ready Active")

```

**√âvolutions importantes :**

1. **Leave forc√© si le worker est d√©j√† dans un Swarm**
    
    Certains tests pr√©c√©dents avaient laiss√© le worker dans un ancien cluster ‚Üí il refusait de joindre le nouveau.
    
    Je force donc :
    
    ```yaml
    docker swarm leave --force
    
    ```
    
    si l‚Äô√©tat n‚Äôest pas `inactive`.
    
2. **Re-v√©rification de l‚Äô√©tat apr√®s le leave**
    
    Je re-lance `docker info` pour garantir que le worker est bien revenu en `inactive` avant de tenter un join.
    
3. **Join avec retry et gestion des erreurs**
    - `retries: 5`, `delay: 5` pour laisser le temps au manager de r√©pondre.
    - `until:` pour r√©essayer tant que √ßa √©choue.
    - `failed_when` pour ignorer proprement le message `already part of a swarm`.
4. **Double validation de l‚Äôadh√©sion au Swarm**
    - C√¥t√© worker : √©tat `active`.
    - C√¥t√© manager : entr√©e dans `docker node ls` avec `Ready Active`.
    
    Cela √©vite le cas o√π le join est ‚Äúlanc√©‚Äù mais jamais finalis√©.
    

---

## Pr√©paration des services : compose.yml + .env adapt√©s √† Swarm

Avant de lancer `docker stack deploy`, j‚Äôai aussi d√ª adapter ma stack pour qu‚Äôelle soit compatible avec Swarm **et** avec la r√©solution DNS entre services.

### Fichier `compose.yml` :

```yaml
version: "3.9"

networks:
  backend:
    driver: overlay

services:
  root_db:
    image: postgres:16
    env_file:
      - .env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U \"$POSTGRES_USER\" -d \"$POSTGRES_DB\""]
      interval: 10s
      timeout: 2s
      retries: 10
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - backend
    deploy:
      placement:
        constraints:
          - node.labels.role == db

  nocodb:
    image: nocodb/nocodb:latest
    env_file:
      - .env
    ports:
      - "8080:8080"
    volumes:
      - nc_data:/usr/app/data
    networks:
      - backend
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.role == app

volumes:
  db_data: {}
  nc_data: {}

```

### Fichier `.env` :

```
POSTGRES_HOST=root_db
POSTGRES_DB=nocodb_db
POSTGRES_USER=admin
POSTGRES_PASSWORD=passwordS√©cur1t√©
NC_DB=pg://root_db:5432?u=admin&p=passwordS√©cur1t√©&d=nocodb_db

```

**Pourquoi ces changements :**

- Ajout d‚Äôun r√©seau overlay `backend` pour que :
    - `nocodb` puisse r√©soudre `root_db` via le DNS interne Swarm.
- Utilisation de `env_file: .env` pour centraliser :
    - la config Postgres (DB, user, password)
    - l‚ÄôURL `NC_DB` utilis√©e par NocoDB.
- Correction de `NC_DB` : avant, elle √©tait vide ou mal form√©e ‚Üí `Invalid URL` puis `ENOTFOUND root_db`.
    
    Maintenant, elle pointe clairement vers `pg://root_db:5432` avec les bons param√®tres.

---

# 4Ô∏è‚É£ Ce que ton playbook Ansible fait maintenant (de bout en bout)

Voici l‚Äôexplication p√©dagogique compl√®te de ton automatisation :

---

## üèÅ √âtape 0 ‚Äî Configuration des hostnames

Tu imposes une coh√©rence syst√®me :

- `app-vm` devient `app-vm`
- `db-vm` devient `db-vm`

C‚Äôest indispensable pour que Swarm ne duplique pas les n≈ìuds.

---

## ‚öôÔ∏è √âtape 1 ‚Äî Installation de Docker

- Mise √† jour apt
- Installation de Docker
- Activation du service

Pr√©-requis absolu pour Swarm et les stacks.

---

## üê≥ √âtape 2 ‚Äî Initialisation du Manager

- V√©rification de l‚Äô√©tat Swarm
- Initialisation si n√©cessaire
- Sauvegarde :
    - token worker
    - token manager
    - IP priv√©e ‚Üí essentielle pour AWS

---

## üß© √âtape 3 ‚Äî Join des workers

V√©rifications robustes :

- leave forc√© si d√©j√† dans un Swarm
- join avec retry
- v√©rification locale
- v√©rification c√¥t√© manager via `docker node ls`

C‚Äôest une √©tape cl√© et tu l‚Äôas s√©curis√©e au maximum.

---

## üè∑Ô∏è √âtape 4 ‚Äî Application des labels

Tu appliques :

- `role=app` sur app-vm
- `role=db` sur db-vm

Avec :

- contr√¥le du NodeID
- commande ex√©cut√©e depuis le manager
- s√©curit√© totale

---

## üöÄ √âtape 5 ‚Äî D√©ploiement de la stack

Tu d√©ploies automatiquement :

- un r√©seau overlay (`backend`)
- Postgres sur le n≈ìud `db`
- NocoDB sur le n≈ìud `app`

Tu prends soin de :

- copier le compose.yml
- copier le .env
- ex√©cuter le deploy
- afficher les services

Cette √©tape est maintenant **stable et reproductible**.

---

# 5Ô∏è‚É£ R√©sultat final

‚úî Swarm fonctionnel

‚úî Manager + Worker op√©rationnels

‚úî Labels appliqu√©s

‚úî R√©seau overlay cr√©√©

‚úî Postgres Running

‚úî NocoDB Running

‚úî D√©ploiement enti√®rement automatis√© via Ansible

‚úî Plus aucune intervention manuelle requise

Tu peux d√©sormais provisionner **n‚Äôimporte quel cluster AWS**, lancer ton playbook et avoir un environnement complet en quelques secondes.

---

# 6Ô∏è‚É£ Conclusion p√©dagogique

üéØ **Ce que tu as r√©ellement accompli :**

Tu as construit une cha√Æne DevOps compl√®te :

- Infrastructure : AWS EC2
- Orchestration : Docker Swarm
- Configuration : Ansible
- R√©seau : overlay distribu√©
- Application : NocoDB + Postgres
- Automatisation : playbook full-stack
- R√©silience : v√©rifications strictes et safe guards
- CI mentale : tu as test√©, logg√©, diagnostiqu√© et corrig√© comme en production

---
[‚Üê Module pr√©c√©dent](M41_projet-J1-swarm.md)
---
