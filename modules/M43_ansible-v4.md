---
title: Projet J3 - Ansible avancÃ©
sujet: Projet
type: module
jour: 43
ordre: 1
tags: projet, docker, docker swarm, stack, ansible
---

# âœ… SynthÃ¨se de la journÃ©e â€“ Stabilisation totale du cluster Docker Swarm & dÃ©ploiement NocoDB

Aujourdâ€™hui, lâ€™Ã©quipe a rÃ©alisÃ© un Ã©norme bond en avant : elle est passÃ©e dâ€™une version instable (v3), oÃ¹ Swarm ne fonctionnait quâ€™alÃ©atoirement, Ã  une **version v4 robuste, cohÃ©rente et totalement automatisÃ©e**, capable de dÃ©ployer un cluster complet avec NocoDB et Postgres de maniÃ¨re fiable.

Cette synthÃ¨se dÃ©crit :

- ce qui posait problÃ¨me dans la v3,
- ce qui a Ã©tÃ© corrigÃ©,
- pourquoi cela a fonctionnÃ©,
- et les versions finales des fichiers.

---

# 1. ğŸš§ Ce qui nâ€™allait pas dans la version v3

## ğŸ”¥ ProblÃ¨me 1 â€” Le manager Swarm nâ€™Ã©tait pas un vrai manager

Lâ€™inventaire v3 dÃ©signait **app-vm** comme â€œmanagerâ€, ce qui Ã©tait incohÃ©rent :

- Le manager se retrouvait sur une VM applicative
- Les workers tentaient de rejoindre un faux manager
- Le cluster Swarm Ã©tait dÃ©truit Ã  chaque tentative de dÃ©ploiement

â¡ï¸ RÃ©sultat : **aucune stabilitÃ© du cluster**.

---

## ğŸ”¥ ProblÃ¨me 2 â€” Les workers nâ€™arrivaient jamais Ã  quitter lâ€™ancien Swarm

Lorsquâ€™un worker avait dÃ©jÃ  un Ã©tat Swarm :

- `docker swarm leave` gÃ©nÃ©rait `context deadline exceeded`
- `/var/lib/docker/swarm` restait verrouillÃ©
- Le worker ne pouvait plus rejoindre le nouveau manager
- Le cluster devenait incohÃ©rent (nÅ“uds fantÃ´mes)

â¡ï¸ **Impossible dâ€™avoir un join propre**.

---

## ğŸ”¥ ProblÃ¨me 3 â€” Le rÃ©seau overlay ne fonctionnait pas

Swarm ne rattachait pas correctement les conteneurs au rÃ©seau overlay.

ConsÃ©quence :

- `root_db` Ã©tait introuvable via DNS
- NocoDB crashait en boucle (`ENOTFOUND root_db`)
- Le rÃ©seau `mystack_backend` nâ€™avait parfois aucun container attachÃ©

â¡ï¸ **Aucune communication entre services**, donc aucun dÃ©marrage applicatif possible.

---

## ğŸ”¥ ProblÃ¨me 4 â€” Les labels et rÃ´les nâ€™Ã©taient pas cohÃ©rents

Les fichiers v3 montraient :

- des labels incorrects
- des rÃ´les non respectÃ©s
- Postgres qui pouvait finir sur un nÅ“ud applicatif
- plusieurs workers surnumÃ©raires ou mal placÃ©s

â¡ï¸ Swarm ne savait pas diffÃ©rencier les VMs â€” donc pas de placement correct.

---

# 2. ğŸ› ï¸ Ce qui a Ã©tÃ© corrigÃ© aujourdâ€™hui

## âœ” 2.1 â€” CrÃ©ation dâ€™un vrai manager dÃ©diÃ©

Lâ€™Ã©quipe a introduit une VM **master-vm** servant exclusivement Ã  :

- initialiser le cluster Swarm
- stocker les tokens worker
- dÃ©ployer la stack
- gÃ©rer les labels
- servir de point dâ€™autoritÃ©

â¡ï¸ Un cluster enfin **stable et prÃ©visible**.

---

## âœ” 2.2 â€” ImplÃ©mentation dâ€™un mÃ©canisme propre de â€œreset Swarmâ€

Lâ€™Ã©quipe a introduit un systÃ¨me intelligent basÃ© sur lâ€™Ã©tat rÃ©el du worker :

### ğŸ” DÃ©tection

- le worker est-il dans un Swarm ?
- est-il reconnu par le manager ?
- existe-t-il une incohÃ©rence ?

### ğŸ”§ Reset uniquement si nÃ©cessaire

- arrÃªt du daemon docker
- suppression de `/var/lib/docker/swarm/*`
- redÃ©marrage docker
- join propre

â¡ï¸ Un worker **ne peut plus rester bloquÃ©** dans un ancien cluster.

---

## âœ” 2.3 â€” RÃ©paration complÃ¨te du rÃ©seau overlay

Les amÃ©liorations :

- crÃ©ation correcte du rÃ©seau `mystack_backend`
- attachement automatique via la stack Swarm
- communication DNS fonctionnelle via `<stack>_<service>`

â¡ï¸ **NocoDB peut enfin joindre Postgres sans erreur DNS.**

---

## âœ” 2.4 â€” RÃ©Ã©criture de lâ€™inventaire et des labels

Chaque rÃ´le est dÃ©sormais :

- lisible
- cohÃ©rent
- utilisÃ© directement pour le placement des services

â¡ï¸ Postgres tourne uniquement sur les nÅ“uds DB

â¡ï¸ NocoDB tourne uniquement sur les nÅ“uds APP

Le cluster est **logique et prÃ©visible**.

---

## âœ” 2.5 â€” Version finale stable et reproductible

AprÃ¨s exÃ©cution du playbook :

- Le cluster est formÃ© correctement
- Tous les workers rejoignent automatiquement
- Les labels sont appliquÃ©s proprement
- Lâ€™image NocoDB est prÃ©-pullÃ©e
- Le rÃ©seau Swarm fonctionne
- NocoDB â†” Postgres communiquent correctement

â¡ï¸ **Le dÃ©ploiement est fiable.**

---

# 3. ğŸ“„ FICHIERS FINAUX (version propre et fonctionnelle)

---

## ğŸ”¹ **inventory.ini (version finale)**

```
[swarm_managers]
master-vm ansible_host=13.39.xx.xx node_labels="role=master"

[swarm_app]
app-vm ansible_host=13.39.xx.xx node_labels="role=app"
app-vm2 ansible_host=13.39.xx.xx node_labels="role=app"
app-vm3 ansible_host=13.39.xx.xx node_labels="role=app"

[swarm_db]
db-vm ansible_host=13.39.xx.xx node_labels="role=db"

[all:vars]
ansible_user=admin
ansible_python_interpreter=/usr/bin/python3

```

---

## ğŸ”¹ **compose.yml (version finale)**

```yaml
version: "3.8"

services:
  nocodb:
    image: registry.gitlab.com/quickdata-batch22-group/nocodb/quickdata/nocodb:0.1.0
    ports:
      - "8080:8080"
    environment:
      - NC_DB=${NC_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.role == app

  root_db:
    image: postgres:16
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - backend
    deploy:
      placement:
        constraints:
          - node.labels.role == db

networks:
  backend:
    driver: overlay

volumes:
  db_data:

```

---

## ğŸ”¹ **swarm-pro.yml (extraits essentiels de la version finale)**

*(Seules les sections clÃ©s sont affichÃ©es pour la synthÃ¨se)*

```yaml
- name: 2. Initialiser le Swarm sur le manager
  hosts: swarm_managers
  tasks:
    - name: VÃ©rifier Ã©tat Swarm
      shell: docker info --format '{{ .Swarm.LocalNodeState }}'
      register: swarm_state
      changed_when: false

    - name: Initialiser Swarm si inactif
      shell: docker swarm init --advertise-addr {{ ansible_host }}
      when: swarm_state.stdout != "active"

    - name: RÃ©cupÃ©rer token worker
      shell: docker swarm join-token -q worker
      register: worker_token

- name: 3. Joindre les workers
  hosts: swarm_app,swarm_db
  tasks:
    - name: Quitter ancien swarm si nÃ©cessaire
      shell: docker swarm leave --force
      when: swarm_state.stdout == "active" and not connu_du_manager

    - name: Join Swarm
      shell: docker swarm join --token {{ hostvars[manager]['worker_token'].stdout }} {{ hostvars[manager]['ansible_host'] }}:2377
      when: worker_pas_membre

- name: 4. Appliquer les labels
  hosts: all
  tasks:
    - name: Ajouter labels
      shell: docker node update --label-add {{ item }} {{ inventory_hostname }}
      loop: "{{ node_labels }}"

- name: 5. DÃ©ployer stack
  hosts: swarm_managers
  tasks:
    - name: Copier compose.yml
      copy:
        src: compose.yml
        dest: /opt/nocodb/compose.yml

    - name: Lancer stack
      shell: docker stack deploy -c /opt/nocodb/compose.yml mystack

```

---

# ğŸ“ **SynthÃ¨se finale**

> Aujourdâ€™hui, lâ€™Ã©quipe a transformÃ© un cluster Swarm instable en une vÃ©ritable plateforme DevOps fiable, auto-rÃ©parable et extensible.
> 
> 
> Elle a corrigÃ© les rÃ´les, les labels, le rÃ©seau overlay, lâ€™initialisation Swarm, les mÃ©canismes de reset et la logique de dÃ©ploiement.
> 
> RÃ©sultat : un environnement capable dâ€™ajouter ou remplacer une VM automatiquement, et de dÃ©ployer NocoDB + Postgres de maniÃ¨re prÃ©dictible.

---
[â† Module prÃ©cÃ©dent](M43_projet-board-J03.md)
---
