---
titre: Test de montÃ©e en charge - Locust & Artillery
type: module
jour: 21
ordre: 1
tags: cd, test, locust, artillery, python, devops
---

# ðŸš€ Cours : Tests de montÃ©e en charge & Framework Locust

> **Objectif du cours** : Comprendre les tests de montÃ©e en charge, savoir les appliquer dans un contexte professionnel ou personnel, et apprendre Ã  utiliser **Locust** (framework Python) pour simuler des utilisateurs et mesurer la rÃ©sistance dâ€™une application.

---

## 1ï¸âƒ£ Introduction : Pourquoi faire des tests de montÃ©e en charge ?

Lorsquâ€™une application est mise en ligne, il est essentiel de **vÃ©rifier sa capacitÃ© Ã  supporter un grand nombre dâ€™utilisateurs simultanÃ©s**.

Les tests de montÃ©e en charge (Load Testing) permettent de :
- Identifier le **seuil critique** de lâ€™application (au-delÃ  duquel elle ralentit ou plante),
- DÃ©tecter les **goulets dâ€™Ã©tranglement** (CPU, RAM, base de donnÃ©es),
- Ã‰valuer la **stabilitÃ©** et la **scalabilitÃ©** du systÃ¨me,
- PrÃ©venir les **pannes** en production.

### ðŸŽ¯ Exemple dâ€™entreprises
- **E-commerce** : gÃ©rer un pic de trafic pendant le Black Friday.  
- **Service SaaS** : supporter 10 000 connexions simultanÃ©es sans crash.  
- **Autoentrepreneur** : vÃ©rifier que son site portfolio reste fluide mÃªme si 100 visiteurs arrivent en mÃªme temps aprÃ¨s une campagne.

---

## 2ï¸âƒ£ Concepts fondamentaux

### ðŸ§± Seuil critique
Une application peut fonctionner correctement avec 100 utilisateurs mais **saturer Ã  1 000**.  
Au-delÃ  du seuil critique, les temps de rÃ©ponse explosent, voire le serveur tombe.

### âš”ï¸ Risques & SÃ©curitÃ©
Une **attaque de dÃ©ni de service (DoS)** exploite ces faiblesses : surcharge du CPU ou de la mÃ©moire, saturation des connexions, etc.

### âš™ï¸ Ressources Ã  surveiller
| Ressource | RÃ´le | SymptÃ´me de surcharge |
|------------|------|------------------------|
| **CPU** | Calculs, traitements logiques | Latence, freeze, erreurs 500 |
| **RAM** | Stockage temporaire des donnÃ©es | Crash, swap disque, lenteur |
| **Disque** | AccÃ¨s aux fichiers/logs | Blocages, IOwait Ã©levÃ©s |
| **RÃ©seau** | Transmission des donnÃ©es | Timeout, goulot dâ€™Ã©tranglement |

### ðŸ’¡ Solutions possibles
- **Optimiser le code** (algorithmes, requÃªtes SQL, cache),
- **Scaling horizontal** (ajouter des instances serveur),
- **Scaling vertical** (augmenter CPU/RAM),
- **SystÃ¨mes de queue** (RabbitMQ, Celery),
- **RÃ©plication** (serveurs ou bases de donnÃ©es).

---

## 3ï¸âƒ£ Simulation dâ€™utilisateurs

Les tests de charge consistent Ã  **simuler des utilisateurs rÃ©els** interagissant avec ton application :
- Navigation sur plusieurs pages,
- Connexion / dÃ©connexion,
- Achats, formulaires, API RESTâ€¦

Ces scÃ©narios permettent de mesurer les performances **dans un contexte rÃ©aliste**.

---

## 4ï¸âƒ£ ðŸ Framework Python : Locust

### Introduction

**Locust** est un framework open-source de tests de charge Ã©crit en Python.  
Il permet de simuler des milliers dâ€™utilisateurs virtuels exÃ©cutant des scÃ©narios dÃ©finis dans un fichier Python (`locustfile.py`).

#### ðŸ”§ CaractÃ©ristiques
- Ã‰crit en **Python** (simple et flexible),
- Interface web intÃ©grÃ©e (`localhost:8089`),
- Compatible avec **GitLab CI/CD**,
- Exporte des rapports en **CSV/HTML**.

---

### Installation et prÃ©requis

#### VÃ©rification Python
```bash
python3 --version
```
> Locust nÃ©cessite Python **â‰¥ 3.7**

#### Installation
```bash
pip install locust
```

---

### CrÃ©ation dâ€™un premier test : `locustfile.py`

Ce fichier contient les scÃ©narios utilisateurs Ã  exÃ©cuter.

```python
from locust import HttpUser, task, between

class FirstLoadTest(HttpUser):
    wait_time = between(1, 3)  # DÃ©lai entre deux requÃªtes

    @task
    def home_page(self):
        self.client.get("/")

    @task
    def about_page(self):
        self.client.get("/about")
```

#### ðŸ” Explication
- `HttpUser` â†’ reprÃ©sente un utilisateur virtuel.  
- `@task` â†’ dÃ©finit une action Ã  rÃ©pÃ©ter pendant le test.  
- `self.client.get()` â†’ effectue une requÃªte HTTP.  
- `wait_time` â†’ simule un dÃ©lai entre deux requÃªtes pour reproduire un comportement humain.

---

### Lancer Locust

#### Commande
```bash
locust
```
âž¡ï¸ Par dÃ©faut, Locust dÃ©marre une interface web sur [http://localhost:8089](http://localhost:8089)

#### Dans lâ€™interface :
- **Host** : URL de ton application (ex. `http://127.0.0.1:5000` ou ton site),
- **Users** : nombre dâ€™utilisateurs simultanÃ©s,
- **Spawn rate** : nouveaux utilisateurs/seconde,
- **Run time** : durÃ©e du test.

Locust va alors simuler la charge et afficher les rÃ©sultats en temps rÃ©el.

---

### Analyse des rÃ©sultats

#### Indicateurs importants
| Indicateur | Description |
|-------------|-------------|
| **RPS (Requests per Second)** | Nombre de requÃªtes traitÃ©es par seconde |
| **Response Time (ms)** | Temps moyen de rÃ©ponse |
| **Fail %** | Taux dâ€™erreur des requÃªtes |
| **Users** | Nombre dâ€™utilisateurs actifs |
| **Throughput** | Volume total de donnÃ©es Ã©changÃ©es |

#### Export des rÃ©sultats
Locust peut exporter les rÃ©sultats :
```bash
locust -f locustfile.py --headless -u 100 -r 10 -t 5m --host=http://localhost:8000 --csv=results
```

---

### IntÃ©gration avec GitLab CI/CD

#### Exemple de pipeline GitLab
```yaml
# .gitlab-ci.yml â€” Test de charge automatisÃ©
stages: [test]

load_test:
  image: python:3.10
  stage: test
  script:
    - pip install locust
    - locust -f locustfile.py --headless -u 50 -r 5 -t 2m --host=http://app:5000 --csv=results
  artifacts:
    paths:
      - results_stats.csv
      - results_failures.csv
  only:
    - main
```

#### ParamÃ¨tres utiles
- `-u` : nombre dâ€™utilisateurs simultanÃ©s,
- `-r` : utilisateurs ajoutÃ©s par seconde,
- `-t` : durÃ©e totale du test,
- `--csv` : export des rÃ©sultats.

#### ðŸ”„ IntÃ©gration continue
Les tests de charge peuvent Ãªtre **dÃ©clenchÃ©s automatiquement** Ã  chaque `merge request`.  
En cas dâ€™Ã©chec, **la fusion est bloquÃ©e** et les rÃ©sultats apparaissent dans GitLab CI/CD.

---

### ðŸ”¬ Exemple dâ€™analyse de rÃ©sultats

| ScÃ©nario | Utilisateurs | Erreurs | Temps moyen | Observation |
|-----------|---------------|----------|--------------|--------------|
| Home Page | 100 | 0% | 150 ms | Stable |
| Login API | 100 | 12% | 1200 ms | RequÃªtes lentes, Ã  optimiser |
| Checkout | 200 | 40% | 2400 ms | Saturation de la base de donnÃ©es |

---

### ðŸ”§ Optimiser aprÃ¨s les tests

1. **Optimiser le code** (requÃªtes SQL, cache, asynchronisme).
2. **Surveiller lâ€™infrastructure** (CPU/RAM, scaling horizontal).
3. **Utiliser du caching** (Redis, CDN).
4. **Mettre en file dâ€™attente** les tÃ¢ches lourdes (RabbitMQ, Celery).
5. **Mettre en place du monitoring** (Prometheus, Grafana, Datadog).

---

### ðŸ’¡ Bonnes pratiques

- Toujours tester **dans un environnement isolÃ©** (prÃ©-prod ou staging).  
- Ne jamais lancer un test massif **sur la prod** sans validation.  
- Analyser les logs systÃ¨me et applicatifs pendant les tests.  
- Automatiser les tests de charge dans le pipeline CI/CD.  
- Garder des **rapports historiques** pour comparer les versions.

---

### ðŸ§  Ã€ retenir

> Les tests de montÃ©e en charge permettent dâ€™anticiper les problÃ¨mes de performance **avant** quâ€™ils nâ€™impactent les utilisateurs rÃ©els.  
> Locust offre un moyen simple, rapide et Pythonique dâ€™automatiser ces tests et de les intÃ©grer Ã  un cycle DevOps complet.

---

### ðŸ“Ž Commandes rÃ©capitulatives

#### Interface Web (par dÃ©faut)

```bash
locust -f locustfile.py --host http://127.0.0.1:8000

```

- Ouvre lâ€™UI sur **http://localhost:8089**
- Tu saisis **Users**, **Spawn rate** et tu lances depuis le navigateur

#### Mode headless (sans UI) + rÃ©sumÃ© final uniquement

```bash
locust -f locustfile.py --headless --only-summary \
  -u 100 -r 10 -t 2m --host http://127.0.0.1:8000

```

---

### ðŸ§© Options essentielles

#### Cible & scÃ©nario

- `f, --locustfile PATH` : fichier test (par dÃ©faut `locustfile.py`)
- `-host URL` : URL cible (peut aussi Ãªtre dÃ©finie dans le code)

#### Charge & durÃ©e

- `u, --users N` : nombre dâ€™utilisateurs simulÃ©s
- `r, --spawn-rate N` : nouveaux utilisateurs par seconde
- `t, --run-time D` : durÃ©e totale (ex : `30s`, `2m`, `1h`)
- `-stop-timeout S` : arrÃªt **gracieux** des users (sec) Ã  la fin

#### Sortie & rapports

- `-headless` : exÃ©cution sans interface web
- `-only-summary` : **nâ€™affiche que le rÃ©sumÃ© final**
- `-csv PREFIX` : export CSV (`PREFIX_stats.csv`, `PREFIX_failures.csv`, â€¦)
- `-csv-full-history` : CSV avec chronologie complÃ¨te (timeseries)
- `-html REPORT.html` : gÃ©nÃ¨re un **rapport HTML** Ã  la fin

#### Logs

- `-loglevel LEVEL` : `INFO`, `DEBUG`, `WARNING`, â€¦
- `-logfile FILE` : envoie les logs dans un fichier

#### UI Web (quand tu veux la garder mais lâ€™exposer ailleurs)

- `-web-host 0.0.0.0` : Ã©coute sur toutes les interfaces
- `-web-port 8089` : port de lâ€™UI

---

### ðŸŽ›ï¸ Filtrer/organiser les tÃ¢ches

> Marque tes tÃ¢ches avec @tag("login"), @tag("checkout") dans le code.
> 
- `-tags login,checkout` : **inclure** seulement ces tags
- `-exclude-tags slow,admin` : **exclure** ces tags

---

### ðŸ§ª ScÃ©narios concrets (recettes)

#### 1) Petit test de fumÃ©e (CI rapide)

```bash
locust -f locustfile.py --headless --only-summary \
  -u 20 -r 5 -t 1m --host http://127.0.0.1:8000

```

#### 2) Campagne avec rapport HTML + CSV complet

```bash
locust -f locustfile.py --headless \
  -u 200 -r 20 -t 10m --host https://app.example.com \
  --csv results/run_$(date +%F_%H%M) --csv-full-history \
  --html report_$(date +%F_%H%M).html

```

#### 3) Test sur un sous-ensemble de tÃ¢ches (tags)

```bash
locust -f locustfile.py --headless --only-summary \
  -u 100 -r 10 -t 5m --host https://api.example.com \
  --tags login,search

```

#### 4) UI exposÃ©e Ã  distance (docker/vm)

```bash
locust -f locustfile.py --host http://service:8000 \
  --web-host 0.0.0.0 --web-port 8089

```

---

### ðŸ§® ExÃ©cution distribuÃ©e (maÃ®tre / travailleurs)

> Pour pousser plus de charge, dÃ©marre 1 master + N workers.
> 

**Master :**

```bash
locust -f locustfile.py --master --headless \
  -u 1000 -r 100 -t 15m --host https://app.example.com --only-summary

```

**Workers :**

```bash
locust -f locustfile.py --worker --master-host 127.0.0.1
# (rÃ©pÃ©ter la commande sur plusieurs machines/containers)

```

Options utiles cÃ´tÃ© master :

- `-expect-workers N` : attend N workers avant de dÃ©marrer
- `-master-bind-host/--master-bind-port` : Ã©coute master personnalisÃ©e

---

### ðŸ§  Petits rappels utiles

- **Users (`u`)** = plateau de charge cible, **Spawn rate (`r`)** = pente de montÃ©e.
- **`-only-summary`** garde la console propre en CI (un verdict clair).
- Toujours fixer `-host` ou le dÃ©finir dans `HttpUser.host`.
- Pense Ã  `-stop-timeout` pour une fin de test propre (ex : 30 s).
- Combine `-csv` et `-html` pour conserver des **preuves** et **comparer** les runs.

---

## 5ï¸âƒ£ Artillery â€” Tests de montÃ©e en charge (complÃ©ment Ã  Locust)

> Objectif : dÃ©couvrir Artillery (outil Node.js) pour le load testing, comprendre quand lâ€™utiliser vs Locust, savoir Ã©crire un fichier unique de test (config + scenarios), lâ€™exÃ©cuter en CLI et lâ€™intÃ©grer Ã  GitLab CI.
> 

### ðŸ“Œ Pourquoi Artillery ?

- **YAML/JS simple** pour dÃ©crire **phases** (arrivÃ©e dâ€™utilisateurs, ramp-up) + **scÃ©narios** (flows HTTP).
- **Seuils de perf** (p95/p99, codes HTTP, Apdex) qui **font Ã©chouer** la CI si non tenus.
- **Rapports** JSON â†’ HTML intÃ©grÃ©s.
- Ã‰cosystÃ¨me Node : facile Ã  installer/embarquer (`npx artillery`), scripts NPM, monorepos JS.

---

### âš”ï¸ Locust vs Artillery â€” que choisir ?

| CritÃ¨re | **Locust** (Python) | **Artillery** (Node.js) |
| --- | --- | --- |
| Langage / Ã©cosystÃ¨me | Python (tests en code Python) | Node.js (YAML ou JS) |
| ModÃ©lisation du user | Code Python (classe `HttpUser`, `@task`) | YAML/JS dÃ©claratif (flows, loops, payloads) |
| DÃ©marrage rapide | UI web locale + CLI headless | CLI `npx artillery run` |
| Seuils / SLO | Via scripts/CI + plugins | **Plugins `ensure`, `apdex`** (natifs) |
| Rapports | CSV/HTML (plugins/outils) | JSON â†’ **HTML** (natif) |
| DistribuÃ© | Master/Workers (trÃ¨s solide) | Via runners/Cloud (ou orchestrÃ© cÃ´tÃ© CI) |
| Points forts | ScÃ©narios Python riches, mocks, extensibilitÃ© | YAML concis, **seuils natifs**, CI-friendly |
| Points de vigilance | Python requis dans lâ€™image CI | Node requis; syntaxe YAML stricte |

**RÃ¨gle simple** :

- Tu es **Ã  lâ€™aise en Python** / scÃ©narios dynamiques complexes â†’ **Locust**.
- Tu veux un **YAML autoportant**, seuils intÃ©grÃ©s, et tâ€™es dans un **stack JS** â†’ **Artillery**.

---

### ðŸ› ï¸ Installer Artillery (au choix)

#### 1) Sans rien installer globalement (recommandÃ©)

> Utilise npx (Node â‰¥ 16 conseillÃ©)
> 

```bash
npx artillery@latest --version
npx artillery run loadtest.yml

```

#### 2) En dÃ©pendance de projet (devDependency)

```bash
# npm
npm install --save-dev artillery
# pnpm
pnpm add -D artillery
# yarn
yarn add -D artillery

```

Ajoute un script dans `package.json` :

```json
{
  "scripts": {
    "loadtest": "artillery run loadtest.yml",
    "loadreport": "artillery report report.json -o report.html"
  }
}

```

Puis :

```bash
npm run loadtest
npm run loadreport

```

#### 3) Global (si tu veux la commande partout)

```bash
npm install -g artillery@latest
artillery --version
artillery run loadtest.yml

```

#### 4) Docker (pas de Node Ã  installer sur la machine)

```bash
docker run --rm -v "$PWD":/app -w /app node:20 \
  npx artillery@latest run -o report.json loadtest.yml

docker run --rm -v "$PWD":/app -w /app node:20 \
  npx artillery@latest report report.json -o report.html

```

---

#### âœ… VÃ©rifier que tout est OK

```bash
node -v        # idÃ©alement >= 16/18/20
npm -v
npx artillery@latest --version

```

### ðŸ§± Fichier Artillery â€” structure (un seul YAML)

Un fichier **unique** avec **deux blocs** : `config` (cible, phases, plugins, payloadsâ€¦) et `scenarios` (flows).

#### âœ… Exemple complet (ton scÃ©nario intÃ©grÃ© & seuils)

```yaml
# loadtest.yml
config:
  target: http://asciiart.artillery.io:8080
  phases:
    - duration: 60
      arrivalRate: 1
      rampTo: 5
      name: Warm up
    - duration: 60
      arrivalRate: 5
      rampTo: 10
      name: Ramp up
    - duration: 30
      arrivalRate: 10
      rampTo: 30
      name: Spike

  plugins:
    ensure:
      thresholds:
        - http.response_time.p99: 100    # p99 <= 100ms
        - http.response_time.p95: 75     # p95 <= 75ms
        # - http.codes.2xx: 95%          # (selon version: % de 2xx)
    apdex:
      threshold: 100
    metrics-by-endpoint: {}

  http:
    timeout: 2000
    # headers:
    #   Authorization: "Bearer {{ token }}"

scenarios:
  - name: Get 3 animal pictures
    flow:
      - loop:
          - get: { url: "/dino" }
          - get: { url: "/pony" }
          - get: { url: "/armadillo" }
        count: 100

```

#### Variantes utiles

- **Pause (think time)** :
    
    ```yaml
    - think: 2
    
    ```
    
- **Variables** :
    
    ```yaml
    config:
      variables:
        userId: 42
    scenarios:
      - flow:
        - get: { url: "/users/{{ userId }}" }
    
    ```
    
- **Payload CSV** (login) :
    
    ```yaml
    config:
      payload:
        path: users.csv
        fields: ["email","password"]
    scenarios:
      - flow:
        - post:
            url: "/login"
            json: { email: "{{ email }}", password: "{{ password }}" }
    
    ```
    
- **Capture & rÃ©utilisation (token)** :
    
    ```yaml
    - post:
        url: "/login"
        json: { user: "john", pass: "doe" }
        capture:
          - json: "$.token"
            as: token
    - get:
        url: "/me"
        headers:
          Authorization: "Bearer {{ token }}"
    
    ```
    

---

### ðŸ–¥ï¸ CLI Artillery â€” commandes indispensables

#### DÃ©marrer un test

```bash
# Simple
npx artillery run loadtest.yml

# Override de la cible
npx artillery run --target https://staging.example.com loadtest.yml

```

#### Rapports

```bash
# JSON brut
npx artillery run -o report.json loadtest.yml

# Rapport HTML (lisible et partageable)
npx artillery report report.json -o report.html

```

#### Options frÃ©quentes

- `-target URL` : change la cible sans toucher au YAML
- `-output|-o report.json` : export des mÃ©triques brutes
- `-overrides` : patch rapide de `config` (phases, targetâ€¦) via JSON inline
- `-environment` : charger un bloc `environments` si tu en dÃ©finis dans `config`

---

### ðŸ¤ IntÃ©gration GitLab CI â€” job robuste avec seuils & rapport

```yaml
stages: [loadtest]

loadtest:
  stage: loadtest
  image: node:20-bookworm
  variables:
    ARTILLERY_TARGET: "https://app.example.com"  # override optionnel
  before_script:
    - node -v && npm -v
    - npm i -g artillery@latest
  script:
    - artillery run --target "${ARTILLERY_TARGET}" -o report.json loadtest.yml
    - artillery report report.json -o report.html
  artifacts:
    when: always
    paths:
      - report.json
      - report.html
    expire_in: 7 days
  # rules:
  #   - if: '$CI_COMMIT_BRANCH == "main"'
  #   - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'

```

- Les **seuils** dans `plugins.ensure.thresholds` donnent un **exit code â‰  0** si non respectÃ©s â†’ **le job Ã©choue** (parfait pour bloquer une MR).
- Ajoute un **job â€œsmokeâ€** (phases courtes) + un **job â€œloadâ€** (plus long) pour sÃ©parer validation rapide et campagne lourde.

---

### ðŸŒ â€œVia le siteâ€ â€” que propose Artillery Cloud ?

- Lancement **hÃ©bergÃ©** et **distribuÃ©** de tests, sans gÃ©rer dâ€™infra.
- **Dashboards** interactifs, historisation des runs, comparaisons de versions.
- **Planification** de campagnes rÃ©currentes (ex: nightly), **alertes**.
- IntÃ©grations CI/CD (GitHub, GitLab) & SLO/thresholds centralisÃ©s.

> Utile si tu veux scaler sans monter toi-mÃªme des runners distribuÃ©s.
> 

---

### ðŸ§­ Quand prÃ©fÃ©rer lâ€™un ou lâ€™autre ?

- **Locust** : tu veux **scripter en Python**, faire du **scÃ©nario complexe**, manipuler des libs Python (auth JWT custom, data science, etc.).
- **Artillery** : tu veux **YAML dÃ©claratif** + **seuils natifs** + **rapport HTML**, et tu es dÃ©jÃ  dans un **stack JS** (Node/Vite/Next).

---

### ðŸ§ª Recettes express

#### 1) Smoke test (CI rapide)

```bash
npx artillery run --target http://127.0.0.1:8000 loadtest.yml

```

#### 2) Campagne avec rapport HTML

```bash
npx artillery run -o report.json loadtest.yml \
 && npx artillery report report.json -o report.html

```

#### 3) MÃªme YAML, cibles diffÃ©rentes (staging/prod)

```bash
npx artillery run --target https://staging.example.com loadtest.yml
npx artillery run --target https://example.com loadtest.yml

```

---

### ðŸ§  Ã€ retenir

> Artillery brille par son YAML concis, ses seuils natifs et ses rapports faciles Ã  partager.
> 
> **Locust** excelle quand le **code Python** est la meilleure faÃ§on dâ€™exprimer un scÃ©nario.
> 
> Garde **un smoke test** court en CI et **une campagne** plus lourde en planifiÃ©e â€” avec des **seuils** qui **font foi**.
>


---


### ðŸŽ¯ Pour aller plus loin
- [Documentation officielle Locust](https://docs.locust.io)
- [Exemples GitLab CI/CD Load Testing](https://docs.gitlab.com/ee/ci/testing/load_performance_testing.html)
- [Tutoriel vidÃ©o Locust & Python](https://www.youtube.com/results?search_query=locust+python+tutorial)